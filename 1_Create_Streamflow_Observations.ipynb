{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "833dd193-a98b-4763-b9e1-ad6e0678d7dc",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6496f06-33b0-4251-8c74-69ec4c0f746f",
   "metadata": {},
   "source": [
    "Critical in evaluation of the NHM model simulated flows is comparison to observed flows in the watershed. This notebook retrieves available streamflow observations from NWIS and two state agencies, the Oregon Water Resources Deparment (OWRD) and the Washington Departent of Ecology (ECY), combines these data sets into one streamflow obervations file and streanflow gage information and metadata, and write the database out as a netCDF file (.nc) to be used in Notebook \"6_Streamflow_Output_Visualization\" and other notebooks in NHM-Assist. A complete database of streamflow gages and observation in the model domain is nessessary to evaluate the NHM model and identify other gages that could be included in a model recalibration to improve the model performance.\n",
    "\n",
    "Three key facts about streamflow observations and the NHM must be reviewed.\n",
    "- Streamflow observations are NOT used when running PRMS or pywatershed. These data are meant for comparison of simulated output only.\n",
    "- The NHM DOES use streamflow observations from NWIS in the model calibration workflow (not the streamflow file).\n",
    "- Limited streamflow gage information is stored in the parameter file.\n",
    "\n",
    "The paramter file has  are dimensioned by npoigages and include :\n",
    "- poi_gage_id, the agency identification number\n",
    "- poi_gage_segment, model segment identification number (nhm_seg) on which the gage falls (1 gage/segment only),\n",
    "- poi_type, historically used, but not currently used.\n",
    "\n",
    "It is important to note that the gages in the parameter file are NOT a complete set of gages in the model domain, and NOT all used to calibrate the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa55d1-68ef-4373-b5ea-bdce0726271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run \"0a_Workspace_setup.ipynb\"\n",
    "%run \"0b_Create_poi_files.ipynb\"\n",
    "\n",
    "from NHM_helpers.sf_data_retrieval import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f939a-733f-4828-8e0b-46a10755114b",
   "metadata": {},
   "source": [
    "# Get daily streamflow data for gages\n",
    "##### We will not use the list of gages in the model paramter file, but will use the gages lsited in the gages_file.csv. This The reasoning: there may be multiple observation datasets that are associated with a single segment outflow (gage_poi), and, in the parameter file, only one poi_gage can be associated with a segment. We want a streamflow data set that is more inclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05df29-3f38-4714-a427-41a9cb4c56e8",
   "metadata": {},
   "source": [
    "## Retrieve availabale daily streamflow data from Oregon Water Resources Department\n",
    "#### https://apps.wrd.state.or.us/apps/sw/hydro_near_real_time/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d33aea-a7e0-4a63-a75f-ff8518e8fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "owrd_regions = [\"16\", \"17\", \"18\"]\n",
    "\n",
    "if any(item in owrd_regions for item in model_domain_regions):\n",
    "    owrd_domain_txt = f\"; The model domain intersects the Oregon state boundary, and streamflow data from OWRD will be retrieved.\"\n",
    "    if output_netcdf_filename.exists():\n",
    "        pass\n",
    "    else:\n",
    "        owrd_df = create_OR_sf_df(control, model_dir, gages_df)\n",
    "\n",
    "else:\n",
    "    owrd_domain_txt = f\"; The model domain is outside the Oregon state boundary.\"\n",
    "\n",
    "con.print(owrd_domain_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6b617-8bad-4e54-a751-4dee1a6957b5",
   "metadata": {},
   "source": [
    "## Retrieve availabale daily streamflow data from Washing Department of Ecology\n",
    "#### https://waecy.maps.arcgis.com/apps/Viewer/index.html?appid=832e254169e640fba6e117780e137e7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9434224-4ba9-43e0-ab33-8fd7a4bd4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecy_regions = [\"17\"]\n",
    "\n",
    "if any(item in ecy_regions for item in model_domain_regions):\n",
    "    # Ecology gages in the gage list have a gage ID with 6 alpha/numeric characters: 2 numeric - one alpha - 2 numeric.\n",
    "\n",
    "    if output_netcdf_filename.exists():\n",
    "        ecy_domain_txt = f\"The model domain intersects the Washington state boundary, and streamflow data from ECY was retrieved.\"\n",
    "        pass\n",
    "    else:\n",
    "        ecy_df = create_ecy_sf_df(control, model_dir, gages_df)\n",
    "        ecy_domain_txt = f\"The model domain intersects the Washington state boundary, and streamflow data for {len(ecy_df)}from ECY was retrieved.\"\n",
    "else:\n",
    "    ecy_domain_txt = \"The model domain is outside the Washington state boundary.\"\n",
    "\n",
    "con.print(ecy_domain_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f5f52-4717-469a-af0e-efe6a7eb5b57",
   "metadata": {},
   "source": [
    "Note: ecy_ds is going outside of the date range. use the time slice using ecy_start_date and ecy_end_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1ebd2-1623-4350-993c-cb0ee666a377",
   "metadata": {},
   "source": [
    "# Retrieve availabale daily streamflow data from NWIS\n",
    "#### https://waterdata.usgs.gov/nwis/sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b94419-ca10-4b11-82cb-1a4a1d743684",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    NWIS_df = create_nwis_sf_df(control, model_dir, gages_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d83b65-47c8-4fcb-99a3-e2382932329f",
   "metadata": {},
   "source": [
    "## Combine daily streamflow dataframes (if needed)\n",
    "note: all NWIS data is mirrored the OWRD database without any primary source tags. This section will also etermine the original source or each daily observation and create a tag for each daily record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83541a18-42bc-4391-b93f-3f0f2640ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    streamflow_df = NWIS_df.copy()  # Sets streamflow file to default, NWIS_df\n",
    "\n",
    "    if (\n",
    "        not owrd_df.empty\n",
    "    ):  # If there is an owrd_df, it will be combined with streamflow_df and rewrite the streamflow_df\n",
    "        # Merge NWIS and OWRD\n",
    "        streamflow_df = pd.concat([streamflow_df, owrd_df])  # Join the two datasets\n",
    "        # Drop duplicated indexes, keeping the first occurence (USGS occurs first)\n",
    "        # try following this thing: https://saturncloud.io/blog/how-to-drop-duplicated-index-in-a-pandas-dataframe-a-complete-guide/#:~:text=Pandas%20provides%20the%20drop_duplicates(),names%20to%20the%20subset%20parameter.\n",
    "        streamflow_df = streamflow_df[~streamflow_df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    elif (\n",
    "        not ecy_df.empty\n",
    "    ):  # If there is an ecy_df, it will be combined with streamflow_df and rewrite the streamflow_df\n",
    "        streamflow_df = pd.concat([streamflow_df, ecy_df])\n",
    "        streamflow_df = streamflow_df[~streamflow_df.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5177c-8ae3-4750-a464-7f026dbb2ff9",
   "metadata": {},
   "source": [
    "#### Poteniatlly important code later for checking stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40222976-eb9a-4b41-a8b9-a9ef0e708c95",
   "metadata": {},
   "source": [
    "## Create Xarray dataset with streamflow date set (indexed by poi_id and time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5c736-9f60-4789-aa96-71edabef1bd7",
   "metadata": {},
   "source": [
    "## Make and Xarray data set for NWIS, OWRD, and WA Ecology data and encode with station information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5cf100-3148-4700-8451-55145bad13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    xr_station_info = xr.Dataset.from_dataframe(\n",
    "        gages_df\n",
    "    )  # gages_df is the new source of gage metadata\n",
    "    xr_streamflow_only = xr.Dataset.from_dataframe(streamflow_df)\n",
    "    xr_streamflow = xr.merge(\n",
    "        [xr_streamflow_only, xr_station_info], combine_attrs=\"drop_conflicts\"\n",
    "    )\n",
    "    # test_poi = xr_streamflow.poi_id.values[2]\n",
    "\n",
    "    # xr_streamflow.agency_id.sel(poi_id=test_poi).to_dataframe().agency_id.unique()\n",
    "    xr_streamflow = xr_streamflow.sortby(\"time\", ascending=True)  # bug fix for xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd09d459-57ad-4f0a-b14c-1deb0e74f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    # Set attributes for the variables\n",
    "    xr_streamflow[\"discharge\"].attrs = {\"units\": \"ft3 s-1\", \"long_name\": \"discharge\"}\n",
    "    xr_streamflow[\"drainage_area\"].attrs = {\n",
    "        \"units\": \"mi2\",\n",
    "        \"long_name\": \"Drainage Area\",\n",
    "    }\n",
    "    xr_streamflow[\"drainage_area_contrib\"].attrs = {\n",
    "        \"units\": \"mi2\",\n",
    "        \"long_name\": \"Effective drainage area\",\n",
    "    }\n",
    "    xr_streamflow[\"latitude\"].attrs = {\n",
    "        \"units\": \"degrees_north\",\n",
    "        \"long_name\": \"Latitude\",\n",
    "    }\n",
    "    xr_streamflow[\"longitude\"].attrs = {\n",
    "        \"units\": \"degrees_east\",\n",
    "        \"long_name\": \"Longitude\",\n",
    "    }\n",
    "    xr_streamflow[\"poi_id\"].attrs = {\n",
    "        \"role\": \"timeseries_id\",\n",
    "        \"long_name\": \"Point-of-Interest ID\",\n",
    "        \"_Encoding\": \"ascii\",\n",
    "    }\n",
    "    xr_streamflow[\"poi_name\"].attrs = {\n",
    "        \"long_name\": \"Name of POI station\",\n",
    "        \"_Encoding\": \"ascii\",\n",
    "    }\n",
    "    xr_streamflow[\"time\"].attrs = {\"standard_name\": \"time\"}\n",
    "    xr_streamflow[\"poi_agency\"].attrs = {\"_Encoding\": \"ascii\"}\n",
    "    xr_streamflow[\"agency_id\"].attrs = {\"_Encoding\": \"ascii\"}\n",
    "\n",
    "    # Set encoding\n",
    "    # See 'String Encoding' section at https://crusaderky-xarray.readthedocs.io/en/latest/io.html\n",
    "    xr_streamflow[\"poi_id\"].encoding.update(\n",
    "        {\"dtype\": \"S15\", \"char_dim_name\": \"poiid_nchars\"}\n",
    "    )\n",
    "\n",
    "    xr_streamflow[\"time\"].encoding.update(\n",
    "        {\n",
    "            \"_FillValue\": None,\n",
    "            \"calendar\": \"standard\",\n",
    "            \"units\": \"days since 1940-01-01 00:00:00\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    xr_streamflow[\"latitude\"].encoding.update({\"_FillValue\": None})\n",
    "    xr_streamflow[\"longitude\"].encoding.update({\"_FillValue\": None})\n",
    "\n",
    "    xr_streamflow[\"agency_id\"].encoding.update(\n",
    "        {\"dtype\": \"S5\", \"char_dim_name\": \"agency_nchars\"}\n",
    "    )\n",
    "\n",
    "    xr_streamflow[\"poi_name\"].encoding.update(\n",
    "        {\"dtype\": \"S50\", \"char_dim_name\": \"poiname_nchars\"}\n",
    "    )\n",
    "\n",
    "    xr_streamflow[\"poi_agency\"].encoding.update(\n",
    "        {\"dtype\": \"S5\", \"char_dim_name\": \"mro_nchars\", \"_FillValue\": \"\"}\n",
    "    )\n",
    "    # Add fill values to the data variables\n",
    "    var_encoding = dict(_FillValue=netCDF4.default_fillvals.get(\"f4\"))\n",
    "\n",
    "    for cvar in xr_streamflow.data_vars:\n",
    "        if xr_streamflow[cvar].dtype != object and cvar not in [\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "        ]:\n",
    "            xr_streamflow[cvar].encoding.update(var_encoding)\n",
    "\n",
    "    # add global attribute metadata\n",
    "    xr_streamflow.attrs = {\n",
    "        \"Description\": \"Streamflow data for PRMS\",\n",
    "        \"FeatureType\": \"timeSeries\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0704a-9d3c-43a0-afd6-39648728430a",
   "metadata": {},
   "source": [
    "## Assign EFC values to the Xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e655ba-b41c-4329-8880-e1d36f9bbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes for the EFC-related variables\n",
    "attributes = {\n",
    "    \"efc\": {\n",
    "        \"dtype\": np.int32,\n",
    "        \"attrs\": {\n",
    "            \"long_name\": \"Extreme flood classification\",\n",
    "            \"_FillValue\": -1,\n",
    "            \"valid_range\": [1, 5],\n",
    "            \"flag_values\": [1, 2, 3, 4, 5],\n",
    "            \"flag_meanings\": \"large_flood small_flood high_flow_pulse low_flow extreme_low_flow\",\n",
    "        },\n",
    "    },\n",
    "    \"ri\": {\n",
    "        \"dtype\": np.float32,\n",
    "        \"attrs\": {\n",
    "            \"long_name\": \"Recurrence interval\",\n",
    "            \"_FillValue\": 9.96921e36,\n",
    "            \"units\": \"year\",\n",
    "        },\n",
    "    },\n",
    "    \"high_low\": {\n",
    "        \"dtype\": np.int32,\n",
    "        \"attrs\": {\n",
    "            \"long_name\": \"Discharge classification\",\n",
    "            \"_FillValue\": -1,\n",
    "            \"valid_range\": [1, 3],\n",
    "            \"flag_values\": [1, 2, 3],\n",
    "            \"flag_meanings\": \"low_flow ascending_limb descending_limb\",\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd91918-d903-41c3-aba9-bd88ecb1eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    with xr.open_dataset(output_netcdf_filename) as model_output:\n",
    "        xr_streamflow = model_output\n",
    "        del model_output\n",
    "else:\n",
    "    var_enc = {}\n",
    "    for var, info in attributes.items():\n",
    "        # Add the variable\n",
    "        xr_streamflow[var] = xr.zeros_like(\n",
    "            xr_streamflow[\"discharge\"], dtype=info[\"dtype\"]\n",
    "        )\n",
    "\n",
    "        var_enc[var] = {\"zlib\": True, \"complevel\": 2}\n",
    "\n",
    "        # Take care of the attributes\n",
    "        del xr_streamflow[var].attrs[\"units\"]\n",
    "\n",
    "        for kk, vv in info[\"attrs\"].items():\n",
    "            if kk == \"_FillValue\":\n",
    "                var_enc[var][kk] = vv\n",
    "            else:\n",
    "                xr_streamflow[var].attrs[kk] = vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a2efd-9a3f-487e-9c1d-e00a4475377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "\n",
    "    flow_col = \"discharge\"\n",
    "\n",
    "    for pp in xr_streamflow.poi_id.data:\n",
    "        try:\n",
    "            df = efc(\n",
    "                xr_streamflow.discharge.sel(poi_id=pp).to_dataframe(), flow_col=flow_col\n",
    "            )\n",
    "\n",
    "            # Add EFC values to the xarray dataset for the poi\n",
    "            xr_streamflow[\"efc\"].sel(poi_id=pp).data[:] = df.efc.values\n",
    "            xr_streamflow[\"high_low\"].sel(poi_id=pp).data[:] = df.high_low.values\n",
    "            xr_streamflow[\"ri\"].sel(poi_id=pp).data[:] = df.ri.values\n",
    "        except TypeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656dfc8-a21c-47c8-9d3c-b393248d66f8",
   "metadata": {},
   "source": [
    "## Write the Xarray data set to a netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc6695-fb2f-440d-b45e-a265a423abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    con.print(\n",
    "        f\"The output file already exists. [bold]To re-write[/] the file, please [bold]remove[/] the existing file:\"\n",
    "    )\n",
    "    con.print(f\"[bright_magenta]{output_netcdf_filename}[/]\")\n",
    "else:\n",
    "    xr_streamflow.to_netcdf(output_netcdf_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a9c0d-8972-4143-a2d5-40a77fe295e5",
   "metadata": {},
   "source": [
    "### Check file: plot individual POI with EFC highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b73ff-8486-42d5-a2d6-abc680aadd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpoi_id = xr_streamflow.poi_id.values[0]\n",
    "\n",
    "ds_sub = xr_streamflow.sel(poi_id=cpoi_id, time=slice(\"1980-10-01\", \"2022-12-31\"))\n",
    "ds_sub = ds_sub.to_dataframe()\n",
    "flow_col = \"discharge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdefab-c24d-4b75-ba30-1fff6c66be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_efc(ds_sub, flow_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0634e13-d764-4b53-8c08-7899b8e01d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_high_low(ds_sub, flow_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b04cd3a-a21a-4d1e-b2ef-94adbfad82f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
