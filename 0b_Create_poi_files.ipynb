{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a599e-9923-40dd-a135-fdde2557fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"0a_Workspace_setup.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a478bf-9417-4ad1-b9ae-e3e10471c416",
   "metadata": {},
   "source": [
    "## 1. Read NHM Subbasin HRU and Segment geodatabases.\n",
    "This reads three layers (<b>nhru, amd nsegments</b>) into GeoPandas as DataFrames (_df) and if geometry is included (_gdb).\n",
    "<b>Note:</b> Layer npoigages includes the poi gages that were included in the model and are limited. Since pois will be added to the model paramter file, we provide another method of for retrieving poi metadata, such as latitude (lat) and longitude (lon), for pois listed in the parameter file that uses NWIS and a supplimental gage ref table for gages that do not occur in NWIS. Locations may NOT be located exactly on the NHM segment. The POIs' assigned segment is displayed in the popup window when the gage icon is clicked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875bc51f-dfdd-4648-b798-3d96b243956c",
   "metadata": {},
   "source": [
    "#### Coordinate system and projection housekeeping\n",
    "Note: Projections are inherited with geometry from the HRUs geodatabase. The NHM uses the **NAD 1983 USGS Contiguous USA Albers** projection EPSG# 102039. The geometry units of this projection are not useful for many notebook packages. The **geodatabases are  reprojected to World Geodetic System 1984**.<br>\n",
    "<br>\n",
    "Options:\n",
    "- crs = 3857, WGS 84 / Pseudo-Mercator - Spherical Mercator, Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI.<br> is also an option\n",
    "- crs = 4326, WGS 84 - WGS84 - World Geodetic System 1984, used in GPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5f875f-3752-491b-ade9-0d84f6ba24bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crs = 4326  # define the new projection that will give lat/lon in units needed for folium mapping\n",
    "# print(poi_gdb.crs) #check the projection of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be13534-cd82-437d-88e5-815b33d0ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the HRU geodatabase\n",
    "if GIS_format == \".gpkg\":\n",
    "    hru_gdb = gpd.read_file(\n",
    "        f\"{model_dir}/GIS/model_layers.gpkg\", layer=\"nhru\"\n",
    "    )  # Reads HRU file to Geopandas.\n",
    "\n",
    "if GIS_format == \".shp\":\n",
    "    hru_gdb = gpd.read_file(\n",
    "        f\"{model_dir}/GIS/model_nhru.shp\"\n",
    "    )  # Reads HRU file to Geopandas.\n",
    "    hru_gdb = hru_gdb.set_index(\"nhm_id\", drop=False).fillna(\n",
    "        0\n",
    "    )  # Set an index for HRU geodatabase.\n",
    "    hru_gdb.index.name = \"index\"  # Index column must be renamed of the hru\n",
    "\n",
    "hru_gdb = hru_gdb.to_crs(crs)  # reprojects to the defined crs projection\n",
    "\n",
    "# hru_gdb.head(10) #prints the first 10\n",
    "# hru_gdb.sample(10) #prints randam 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7e861-1ca3-4d23-acc5-c0a879ac9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the Segment geodatabase\n",
    "if GIS_format == \".gpkg\":\n",
    "    seg_gdb = gpd.read_file(\n",
    "        f\"{model_dir}/GIS/model_layers.gpkg\", layer=\"nsegment\"\n",
    "    ).fillna(\n",
    "        0\n",
    "    )  # Reads segemnt file to Geopandas.\n",
    "\n",
    "if GIS_format == \".shp\":\n",
    "    seg_gdb = gpd.read_file(f\"{model_dir}/GIS/model_nsegment.shp\").fillna(0)\n",
    "    seg_gdb = seg_gdb.set_index(\n",
    "        \"nhm_seg\", drop=False\n",
    "    )  # Set an index for segment geodatabase(GIS)\n",
    "    seg_gdb.index.name = \"index\"  # Index column must be renamed of the hru\n",
    "\n",
    "seg_gdb = seg_gdb.to_crs(crs)  # reprojects to the defined crs projection\n",
    "\n",
    "# seg_gdb.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e7d4b1-02cf-478c-962d-23b851c61917",
   "metadata": {},
   "source": [
    "## 2. Merge HRU parameters from the parameter file with HRU GeoDataFrame\n",
    "Extracts HRU latitude and longitude from the params file using pyPRMS and creates a dataframe. This is useful as a cross-check for HRU location and in later notebook leaflet applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0c4881-cb4d-4053-8e07-4b43229ce9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of parameters values to retrieve for the HRUs.\n",
    "hru_params = [\n",
    "    \"hru_lat\",  # the latitude if the hru centroid\n",
    "    \"hru_lon\",  # the longitude if the hru centroid\n",
    "    \"hru_area\",\n",
    "    \"hru_segment_nhm\",  # The nhm_id of the segment recieving flow from the HRU\n",
    "]  # List any param here that is dimensioned by nHRU. Not intended for by nmonth X nHRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6aeec-63ce-476e-afd5-7e08c1205ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of calibration parameters\n",
    "nhru_params = [\n",
    "    \"carea_max\",\n",
    "    \"emis_noppt\",\n",
    "    \"fastcoef_lin\",\n",
    "    \"freeh2o_cap\",\n",
    "    \"gwflow_coef\",\n",
    "    \"potet_sublim\",\n",
    "    \"rad_trncf\",\n",
    "    \"slowcoef_sq\",\n",
    "    \"smidx_coef\",\n",
    "    \"smidx_exp\",\n",
    "    \"snowinfil_max\",\n",
    "    \"soil2gw_max\",\n",
    "    \"soil_moist_max\",\n",
    "    \"soil_rechr_max_frac\",\n",
    "    \"ssr2gw_exp\",\n",
    "    \"ssr2gw_rate\",\n",
    "]  # List any output param here.\n",
    "\n",
    "nhru_nmonths_params = [\n",
    "    \"adjmix_rain\",\n",
    "    \"cecn_coef\",\n",
    "    \"jh_coef\",\n",
    "    \"tmax_allrain_offset\",\n",
    "    \"tmax_allsnow\",\n",
    "    \"radmax\",\n",
    "    \"rain_cbh_adj\",\n",
    "    \"snow_cbh_adj\",\n",
    "    \"tmax_cbh_adj\",\n",
    "    \"tmin_cbh_adj\",\n",
    "]  # List any output param here.\n",
    "cal_hru_params = nhru_params + nhru_nmonths_params\n",
    "gdb_hru_params = hru_params + nhru_params + nhru_nmonths_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49e019-c794-42e5-8078-f76743bee0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for parameter values\n",
    "first = True\n",
    "for vv in gdb_hru_params:\n",
    "    if (\n",
    "        first\n",
    "    ):  # this creates the first iteration for the following iterations to concantonate to\n",
    "        df = pdb.get_dataframe(vv)\n",
    "        first = False\n",
    "    else:\n",
    "        df = pd.concat([df, pdb.get_dataframe(vv)], axis=1)  # , ignore_index=True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df[\"model_idx\"] = (\n",
    "    df.index + 1\n",
    ")  #'model_idx' created here is the order of the parameters in the parameter file.\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d86c44-cbc8-4832-8850-18df3b8042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the HRU params values to the HRU geodatabase using Merge\n",
    "hru_gdb = pd.merge(df, hru_gdb, on=\"nhm_id\")\n",
    "# hru_gdb.round(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d23f98-cdf2-40be-a338-ab62a32f13f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Goepandas GeoDataFrame for the HRU geodatabase\n",
    "hru_gdf = gpd.GeoDataFrame(\n",
    "    hru_gdb, geometry=\"geometry\"\n",
    ")  # Creates a Geopandas GeoDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc94a91-16fa-4a51-9f22-b1ede42cd8f0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Merge segment parameter values from the parameter file with segment GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcc5e9b-d62c-4439-b1c2-da4a216afadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of parameters values to retrieve for the segments.\n",
    "seg_params = [\"tosegment_nhm\", \"tosegment\", \"seg_length\", \"obsin_segment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295fd984-f862-4196-94ae-75194be68409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for parameter values\n",
    "first = True\n",
    "for vv in seg_params:\n",
    "    if first:\n",
    "        df = pdb.get_dataframe(vv)\n",
    "        first = False\n",
    "    else:\n",
    "        df = pd.concat([df, pdb.get_dataframe(vv)], axis=1)  # , ignore_index=True)\n",
    "\n",
    "df.reset_index(inplace=True)\n",
    "df[\"model_idx\"] = df.index + 1\n",
    "df.index.name = \"index\"  # Index column must be renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5990a7b-da57-4fd9-b162-75a523edb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the HRU params values to the HRU geodatabase using Merge\n",
    "seg_gdb = pd.merge(df, seg_gdb, on=\"nhm_seg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684d492-a5d4-4c59-8adf-01d560e25316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Goepandas GeoDataFrame for the HRU geodatabase\n",
    "seg_gdf = gpd.GeoDataFrame(seg_gdb, geometry=\"geometry\")\n",
    "seg_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f31fa1-ca9d-48ee-b1dc-2d05639edf08",
   "metadata": {},
   "source": [
    "## Create POI DataFrame (poi_df) for gages included in the NHMx parameter file (poi_gages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430bb9c-c255-44f2-baf6-c98aa539a1e1",
   "metadata": {},
   "source": [
    "#### Get POI-related parameters and reshape as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363dccf-f9f0-4625-b5cb-5403deb9bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "poi = pdb[\"poi_gage_id\"].as_dataframe\n",
    "poi = poi.merge(pdb[\"poi_gage_segment\"].as_dataframe, left_index=True, right_index=True)\n",
    "poi = poi.merge(pdb[\"poi_type\"].as_dataframe, left_index=True, right_index=True)\n",
    "poi = poi.merge(\n",
    "    pdb[\"nhm_seg\"].as_dataframe, left_on=\"poi_gage_segment\", right_index=True\n",
    ")\n",
    "# poi.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1910c0f7-8c5b-44ca-b4c1-94e619f3f63f",
   "metadata": {},
   "source": [
    "#### Get poi metadata from NWIS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb95635-c0b8-468c-960e-2ae90e00be24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import datetime\n",
    "import re\n",
    "from io import StringIO\n",
    "import sys\n",
    "from collections import OrderedDict\n",
    "from urllib.request import urlopen, Request\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5171ac6-ab1e-4f8e-abbb-e8fc10ea9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start and end dates are for the date span for gage activity.\n",
    "st_date = datetime.datetime(1930, 1, 1)\n",
    "en_date = datetime.datetime(2022, 12, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9554e-4a10-42c1-a1f1-c6ba00af6f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NWIS data pull functions from pyPRMS\n",
    "base_url = \"http://waterservices.usgs.gov/nwis\"\n",
    "\n",
    "t1 = re.compile(\"^#.*$\\n?\", re.MULTILINE)  # remove comment lines\n",
    "t2 = re.compile(\"^5s.*$\\n?\", re.MULTILINE)  # remove field length lines\n",
    "\n",
    "\n",
    "def get_nwis_site_fields():\n",
    "    \"\"\"Get NWIS streamgage field information.\n",
    "\n",
    "    :returns: dictionary of field_name, datatype pairs\n",
    "    \"\"\"\n",
    "    # Retrieve a single station and pull out the field names and data types\n",
    "    stn_url = (\n",
    "        f\"{base_url}/site/?format=rdb&sites=01646500&siteOutput=expanded&\"\n",
    "        + \"siteStatus=active&parameterCd=00060&siteType=ST\"\n",
    "    )\n",
    "\n",
    "    response = urlopen(stn_url)\n",
    "    encoding = response.info().get_param(\"charset\", failobj=\"utf8\")\n",
    "    streamgage_site_page = response.read().decode(encoding)\n",
    "\n",
    "    # Strip the comment lines and field length lines from the result\n",
    "    streamgage_site_page = t1.sub(\"\", streamgage_site_page, 0)\n",
    "\n",
    "    # nwis_dtypes = t2.findall(streamgage_site_page)[0].strip('\\n').split('\\t')\n",
    "    nwis_fields = StringIO(streamgage_site_page).getvalue().split(\"\\n\")[0].split(\"\\t\")\n",
    "\n",
    "    nwis_final = {}\n",
    "    for fld in nwis_fields:\n",
    "        code = fld[-2:]\n",
    "        if code in [\"cd\", \"no\", \"nm\", \"dt\"]:\n",
    "            nwis_final[fld] = np.str_\n",
    "        elif code in [\"va\"]:\n",
    "            nwis_final[fld] = np.float32\n",
    "        else:\n",
    "            nwis_final[fld] = np.str_\n",
    "\n",
    "    return nwis_final\n",
    "\n",
    "\n",
    "def _retrieve_from_nwis(url):\n",
    "    \"\"\"Get streamgage site page for given URL.\n",
    "\n",
    "    :param url: URL to streamgage page on NWIS\n",
    "    :returns: streamgage site page\n",
    "    \"\"\"\n",
    "    response = urlopen(url)\n",
    "    encoding = response.info().get_param(\"charset\", failobj=\"utf8\")\n",
    "    streamgage_site_page = response.read().decode(encoding)\n",
    "\n",
    "    # Strip the comment lines and field length lines from the result\n",
    "    streamgage_site_page = t1.sub(\"\", streamgage_site_page, 0)\n",
    "    streamgage_site_page = t2.sub(\"\", streamgage_site_page, 0)\n",
    "\n",
    "    return streamgage_site_page\n",
    "\n",
    "\n",
    "def get_nwis_sites(stdate, endate, sites=None, regions=None):\n",
    "    \"\"\"Get NWIS streamgage site information\n",
    "\n",
    "    :param stdate: start date for extraction\n",
    "    :param endate: end date for extraction\n",
    "    :param sites: streamgage(s) to pull from NWIS\n",
    "    :param regions: region(s) to pull from NWIS\n",
    "    :returns: dataframe of streamgage site information\n",
    "    \"\"\"\n",
    "    cols = get_nwis_site_fields()\n",
    "\n",
    "    # Columns to include in the final dataframe\n",
    "    include_cols = [\n",
    "        \"agency_cd\",\n",
    "        \"site_no\",\n",
    "        \"station_nm\",\n",
    "        \"dec_lat_va\",\n",
    "        \"dec_long_va\",\n",
    "        \"dec_coord_datum_cd\",\n",
    "        \"alt_va\",\n",
    "        \"alt_datum_cd\",\n",
    "        \"huc_cd\",\n",
    "        \"drain_area_va\",\n",
    "        \"contrib_drain_area_va\",\n",
    "    ]\n",
    "\n",
    "    # Start with an empty dataframe\n",
    "    nwis_sites = pd.DataFrame(columns=include_cols)\n",
    "\n",
    "    url_pieces = OrderedDict()\n",
    "    url_pieces[\"format\"] = \"rdb\"\n",
    "    url_pieces[\"startDT\"] = stdate.strftime(\"%Y-%m-%d\")\n",
    "    url_pieces[\"endDT\"] = endate.strftime(\"%Y-%m-%d\")\n",
    "    # url_pieces['huc'] = None\n",
    "    url_pieces[\"siteOutput\"] = \"expanded\"\n",
    "    url_pieces[\"siteStatus\"] = \"all\"\n",
    "    url_pieces[\"parameterCd\"] = \"00060\"  # Discharge\n",
    "    url_pieces[\"siteType\"] = \"ST\"\n",
    "    # url_pieces['hasDataTypeCd'] = 'dv'\n",
    "\n",
    "    # NOTE: If both sites and regions parameters are specified the sites\n",
    "    #       parameter takes precedence.\n",
    "    if sites is None:\n",
    "        # No sites specified so default to HUC02-based retrieval\n",
    "        url_pieces[\"huc\"] = None\n",
    "\n",
    "        if regions is None:\n",
    "            # Default to HUC02 regions 1 thru 18\n",
    "            regions = list(range(1, 19))\n",
    "        if isinstance(regions, (list, tuple)):\n",
    "            pass\n",
    "        else:\n",
    "            # Single region\n",
    "            regions = [regions]\n",
    "    else:\n",
    "        # One or more sites were specified\n",
    "        url_pieces[\"sites\"] = None\n",
    "\n",
    "        if isinstance(sites, (list, tuple)):\n",
    "            pass\n",
    "        else:\n",
    "            # Single string, convert to list of sites\n",
    "            sites = [sites]\n",
    "\n",
    "    if \"huc\" in url_pieces:\n",
    "        # for region in range(19):\n",
    "        for region in regions:\n",
    "            sys.stdout.write(f\"\\r  Region: {region:02}\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            url_pieces[\"huc\"] = f\"{region:02}\"\n",
    "            url_final = \"&\".join([f\"{kk}={vv}\" for kk, vv in url_pieces.items()])\n",
    "            stn_url = f\"{base_url}/site/?{url_final}\"\n",
    "\n",
    "            streamgage_site_page = _retrieve_from_nwis(stn_url)\n",
    "\n",
    "            # Read the rdb file into a dataframe\n",
    "            df = pd.read_csv(\n",
    "                StringIO(streamgage_site_page),\n",
    "                sep=\"\\t\",\n",
    "                dtype=cols,\n",
    "                usecols=include_cols,\n",
    "            )\n",
    "\n",
    "            # pandas append deprecated since v1.4\n",
    "            # nwis_sites = nwis_sites.append(df, ignore_index=True)\n",
    "            nwis_sites = pd.concat([nwis_sites, df], ignore_index=True)\n",
    "            sys.stdout.write(\"\\r                      \\r\")\n",
    "    else:\n",
    "        for site in sites:\n",
    "            sys.stdout.write(f\"\\r  Site: {site} \")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            url_pieces[\"sites\"] = site\n",
    "            url_final = \"&\".join([f\"{kk}={vv}\" for kk, vv in url_pieces.items()])\n",
    "\n",
    "            stn_url = f\"{base_url}/site/?{url_final}\"\n",
    "\n",
    "            try:\n",
    "                streamgage_site_page = _retrieve_from_nwis(stn_url)\n",
    "\n",
    "                # Read the rdb file into a dataframe\n",
    "                df = pd.read_csv(\n",
    "                    StringIO(streamgage_site_page),\n",
    "                    sep=\"\\t\",\n",
    "                    dtype=cols,\n",
    "                    usecols=include_cols,\n",
    "                )\n",
    "\n",
    "                # pandas append deprecated since v1.4\n",
    "                # nwis_sites = nwis_sites.append(df, ignore_index=True)\n",
    "                nwis_sites = pd.concat([nwis_sites, df], ignore_index=True)\n",
    "            except HTTPError as err:\n",
    "                if err.code == 404:\n",
    "                    sys.stdout.write(\n",
    "                        f\"HTTPError: {err.code}, site does not meet criteria - SKIPPED\\n\"\n",
    "                    )\n",
    "            sys.stdout.write(\"\\r                      \\r\")\n",
    "\n",
    "    field_map = {\n",
    "        \"agency_cd\": \"poi_agency\",\n",
    "        \"site_no\": \"poi_id\",\n",
    "        \"station_nm\": \"poi_name\",\n",
    "        \"dec_lat_va\": \"latitude\",\n",
    "        \"dec_long_va\": \"longitude\",\n",
    "        \"alt_va\": \"elevation\",\n",
    "        \"drain_area_va\": \"drainage_area\",\n",
    "        \"contrib_drain_area_va\": \"drainage_area_contrib\",\n",
    "    }\n",
    "\n",
    "    nwis_sites.rename(columns=field_map, inplace=True)\n",
    "    nwis_sites.set_index(\"poi_id\", inplace=True)\n",
    "    nwis_sites = nwis_sites.sort_index()\n",
    "\n",
    "    return nwis_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49534463-5cb4-4d1d-9c9a-8b76c4e09e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads/Creates NWIS stations file if not already created\n",
    "if nwis_gages_file.exists():\n",
    "    col_names = [\n",
    "        \"poi_id\",\n",
    "        \"poi_agency\",\n",
    "        \"poi_name\",\n",
    "        \"latitude\",\n",
    "        \"longitude\",\n",
    "        #'dec_coord_datum_cd',\n",
    "        #'elevation',\n",
    "        #'alt_datum_cd',\n",
    "        #'huc_cd',\n",
    "        \"drainage_area\",\n",
    "        \"drainage_area_contrib\",\n",
    "    ]\n",
    "    col_types = [\n",
    "        np.str_,\n",
    "        np.str_,\n",
    "        np.str_,\n",
    "        float,\n",
    "        float,\n",
    "        # np.str_,\n",
    "        # float,\n",
    "        # np.str_,\n",
    "        # np.str_,\n",
    "        float,\n",
    "        float,\n",
    "    ]\n",
    "    cols = dict(\n",
    "        zip(col_names, col_types)\n",
    "    )  # Creates a dictionary of column header and datatype called below.\n",
    "\n",
    "    nwis_gages_aoi = pd.read_csv(\n",
    "        nwis_gages_file,\n",
    "        dtype=cols,\n",
    "        usecols=[\n",
    "            \"poi_id\",\n",
    "            \"poi_agency\",\n",
    "            \"poi_name\",\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "            \"drainage_area\",\n",
    "            \"drainage_area_contrib\",\n",
    "        ],\n",
    "    )  # sep='\\t',\n",
    "else:\n",
    "    nwis_sites = get_nwis_sites(\n",
    "        stdate=st_date, endate=en_date, regions=model_domain_regions\n",
    "    )  # Pulls all sites in conus\n",
    "    # Pull a list of sites\n",
    "    # df1 = get_nwis_sites(stdate=st_date, endate=en_date, sites=['01012500', '01010000'])\n",
    "    # Pull a list of regions\n",
    "    # df1 = get_nwis_sites(stdate=st_date, endate=en_date, regions=['1', '2', '3'])\n",
    "\n",
    "    # Make sites a geodatabase\n",
    "    nwis_sites_gdf = gpd.GeoDataFrame(\n",
    "        nwis_sites,\n",
    "        geometry=gpd.points_from_xy(nwis_sites.longitude, nwis_sites.latitude),\n",
    "        crs=crs,\n",
    "    )\n",
    "\n",
    "    # clip to the model domain\n",
    "    nwis_gages_aoi = nwis_sites_gdf.clip(hru_gdf)\n",
    "    nwis_gages_aoi.reset_index(inplace=True)\n",
    "    nwis_gages_aoi.drop(columns=[\"geometry\"], inplace=True)\n",
    "\n",
    "    # trim off things we don't need\n",
    "    nwis_gages_aoi.drop(\n",
    "        columns=[\n",
    "            \"dec_coord_datum_cd\",\n",
    "            \"elevation\",\n",
    "            \"alt_datum_cd\",\n",
    "            \"huc_cd\",\n",
    "            #'drainage_area',\n",
    "            #'drainage_area_contrib'\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # write out the file for later\n",
    "    nwis_gages_aoi.to_csv(nwis_gages_file, index=False)  # , sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedbe717-9465-4bbe-bac7-7701673cb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nwis_sites_aoi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9f826-1f2e-4f84-b200-e00e0e6c7326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the temp dataframe (NHM poi information from NWIS) with the pois data from the parameter file.\n",
    "poi = poi.merge(nwis_gages_aoi, left_on=\"poi_gage_id\", right_on=\"poi_id\", how=\"left\")\n",
    "poi_df = pd.DataFrame(poi)  # Creates a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2894d-a4bf-49d2-b6a5-ac686ebee48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ae690-f22d-46c9-af17-237604436615",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create default_gages.csv for your model extraction.\n",
    "##### This will be composed of the gages that are in the model, but also include NWIS gages that may be added later to the model. Time-series data for streamflow observations will be collected using this gage list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd47350-20a8-4b5b-87b8-99456be9dfae",
   "metadata": {},
   "source": [
    "#### Make a dataframe of the non-NWIS gages (if present) in the parameter file (poi_gages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93512624-515f-4976-8bb7-05bdc68194d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the gages in the parameter file that are not USGS gages in NWIS\n",
    "if pd.isnull(poi_df[\"poi_agency\"]).values.any():\n",
    "    non_NWIS_gages_from_poi_df = poi_df.loc[pd.isnull(poi_df[\"poi_agency\"])]\n",
    "    non_NWIS_gages_from_poi_df.drop(\n",
    "        columns=[\"poi_id\", \"nhm_seg\", \"poi_gage_segment\", \"poi_type\"], inplace=True\n",
    "    )\n",
    "    non_NWIS_gages_from_poi_df.rename(columns={\"poi_gage_id\": \"poi_id\"}, inplace=True)\n",
    "    # non_NWIS_gages_from_poi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c4eba-8bd3-40f6-8b3a-2043354fef4f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Make a dataframe for all non-NWIS gages (if present) in the parameter file and all NWIS gages in the model domain\n",
    "##### Note: the NWIS gages in the poi_df (gages in the parameter file) should be in NWIS_sites_aoi df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa730ed-b61f-4352-aac0-a6ddaaefad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_file_col_order = [\n",
    "    \"poi_id\",\n",
    "    \"poi_agency\",\n",
    "    \"poi_name\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"drainage_area\",\n",
    "    \"drainage_area_contrib\",\n",
    "    #'nhm_seg', 'poi_gage_segment', 'poi_type'\n",
    "]\n",
    "if pd.isnull(poi_df[\"poi_agency\"]).values.any():\n",
    "    temp = pd.concat([nwis_gages_aoi, non_NWIS_gages_from_poi_df], ignore_index=True)\n",
    "    temp2 = temp[sta_file_col_order]\n",
    "\n",
    "else:\n",
    "    temp = nwis_gages_aoi.copy()\n",
    "    temp2 = temp[sta_file_col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d484280-803f-46fb-b4b1-680c320a0313",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Save the default station file (gage file) as a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690621fc-467f-4efb-8bb6-d87ab88d7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2.to_csv(default_gages_file, index=False)\n",
    "# temp2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952662d6-201f-405e-8e0c-cc9e7d2c9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330c350-62f0-43f5-8cd9-5695f07fa69f",
   "metadata": {},
   "source": [
    "## Create gages.csv file using default_gages.csv file.\n",
    "##### If there are gages in the parameter file that are not in NWIS (USGS gages), then latitude, longitude, and poi_name must be provided from another source, and appended to the \"default_gages.csv\" file. Once editing is complete, that file can be renamed \"gages.csv\"and will be used as the gages file. If NO gages.csv is made, the default_gages.csv will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ccd9c-9f59-4ceb-a789-b68f782bc59a",
   "metadata": {},
   "source": [
    "## Update gages_df if gages.csv was created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27faffa3-91ea-4db7-a2bc-fb43fd9b6991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9854913-2a83-42d4-a3f6-9065773a25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in station file columns needed (You may need to tailor this to the particular file.\n",
    "col_names = [\n",
    "    \"poi_id\",\n",
    "    \"poi_agency\",\n",
    "    \"poi_name\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"drainage_area\",\n",
    "    \"drainage_area_contrib\",\n",
    "]\n",
    "col_types = [np.str_, np.str_, np.str_, float, float, float, float]\n",
    "cols = dict(\n",
    "    zip(col_names, col_types)\n",
    ")  # Creates a dictionary of column header and datatype called below.\n",
    "\n",
    "if gages_file.exists():\n",
    "\n",
    "    nwis_gages_aoi = pd.read_csv(nwis_gages_file, dtype=cols)\n",
    "    gages_df = pd.read_csv(gages_file)\n",
    "\n",
    "    # Make poi_id the index\n",
    "    gages_df.set_index(\"poi_id\", inplace=True)\n",
    "\n",
    "    con.print(f\"The appended gage file will be used and has {len(gages_df)} gages.\")\n",
    "else:\n",
    "    gages_df = pd.read_csv(default_gages_file, dtype=cols)\n",
    "\n",
    "    # Make poi_id the index\n",
    "    gages_df.set_index(\"poi_id\", inplace=True)\n",
    "\n",
    "    con.print(f\"The default gage file will be used and has {len(gages_df)} gages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796099f9-97c2-4aae-b03a-8c8b4e8d658b",
   "metadata": {},
   "source": [
    "## Update the poi_df metadata\n",
    "#### If an improved/edited gages.csv file exists, then the poi_df metadata should also be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8fad55-a0ed-477f-a813-2efc6571c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates the non-usgs gages in the poi dataframe with metadata from the stations file (that was added or edited)\n",
    "if gages_file.exists():\n",
    "    for idx, row in poi_df.iterrows():\n",
    "        if pd.isnull(row[\"poi_id\"]):\n",
    "            new_poi_id = row[\"poi_gage_id\"]\n",
    "            new_lat = gages_df.loc[\n",
    "                gages_df.index == row[\"poi_gage_id\"], \"latitude\"\n",
    "            ].values[0]\n",
    "            new_lon = gages_df.loc[\n",
    "                gages_df.index == row[\"poi_gage_id\"], \"longitude\"\n",
    "            ].values[0]\n",
    "            new_poi_agency = gages_df.loc[\n",
    "                gages_df.index == row[\"poi_gage_id\"], \"poi_agency\"\n",
    "            ].values[0]\n",
    "            new_poi_name = gages_df.loc[\n",
    "                gages_df.index == row[\"poi_gage_id\"], \"poi_name\"\n",
    "            ].values[0]\n",
    "\n",
    "            poi_df.loc[idx, \"latitude\"] = new_lat\n",
    "            poi_df.loc[idx, \"longitude\"] = new_lon\n",
    "            poi_df.loc[idx, \"poi_id\"] = new_poi_id\n",
    "            poi_df.loc[idx, \"poi_agency\"] = new_poi_agency\n",
    "            poi_df.loc[idx, \"poi_name\"] = new_poi_name\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5208f-1414-48f2-b0cd-70fe23976c44",
   "metadata": {},
   "source": [
    "##### All pois in the poi_df with missing data for lat, lon, and poi_name will be dropped from the poi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d273d-d738-4d26-b708-f145172d6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print warning and drop poi's with missing data for lat, lon, and poi_name\n",
    "missing_meta_df = poi_df[\n",
    "    poi_df[[\"latitude\", \"longitude\", \"poi_name\"]].isna().any(axis=1)\n",
    "]  # poi_df[poi_df.isna().any(axis=1)]\n",
    "con.print(\n",
    "    f\"WARNING: Gage {missing_meta_df.poi_gage_id.values} missing metadata and will be dropped from the poi_gdf.\"\n",
    ")\n",
    "# poi_df.notna(inplace=True, ignore_index=False)\n",
    "# poi_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "poi_gdf = gpd.GeoDataFrame(\n",
    "    poi_df,\n",
    "    geometry=gpd.points_from_xy(poi_df.longitude, poi_df.latitude),\n",
    "    crs=crs,\n",
    ").dropna(subset=[\"latitude\", \"longitude\", \"poi_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbd19f-3ea9-4d87-932b-9a414e8ce786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poi_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1f076-d1e6-435d-8c4f-fc245aec0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gages_list = gages_df.index.to_list()\n",
    "nwis_gages_aoi_list = nwis_gages_aoi.poi_id.to_list()\n",
    "nwis_gages_in_gages_list = list(set(nwis_gages_aoi_list) & set(gages_list))\n",
    "con.print(\n",
    "    f\"The gages file has {len(gages_list)} streamflow gages, {len(nwis_gages_in_gages_list)} are in NWIS.\"\n",
    ")\n",
    "\n",
    "con.print(\n",
    "    f\"The parameter file has {len(poi_df)} gages, {len(poi_df) - len(poi_gdf)} are missing metadata and \\ncannot be mapped in NHMassist notebooks unless the gages.csv is modified.\"\n",
    ")\n",
    "\n",
    "additional_gages = list(set(gages_list) - set(poi_df.poi_id))\n",
    "nwis_gages_in_additional_gages_list = list(\n",
    "    set(nwis_gages_aoi_list) & set(additional_gages)\n",
    ")\n",
    "\n",
    "con.print(\n",
    "    f\"An additional {len(additional_gages)} streamflow gages are in the model domain and not in the parameter file; {len(nwis_gages_in_additional_gages_list)} are in NWIS.\"\n",
    ")\n",
    "# nwis_gages_aoi_list = nwis_gages_aoi.poi_id.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44ff45d-942a-4141-90b3-21c09f2a4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "hru_gdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba967cb-1aab-450f-acda-4b5dda1dc6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
