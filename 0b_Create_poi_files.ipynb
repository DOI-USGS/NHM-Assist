{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a599e-9923-40dd-a135-fdde2557fb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"0a_Workspace_setup.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a478bf-9417-4ad1-b9ae-e3e10471c416",
   "metadata": {},
   "source": [
    "## 1. Read NHM Subbasin HRU and Segment geodatabases.\n",
    "This reads three layers (<b>nhru, amd nsegments</b>) into GeoPandas as DataFrames (_df) and if geometry is included (_gdb).\n",
    "<b>Note:</b> Layer npoigages includes the poi gages that were included in the model and are limited. Since pois will be added to the model paramter file, we provide another method of for retrieving poi metadata, such as latitude (lat) and longitude (lon), for pois listed in the parameter file that uses NWIS and a supplimental gage ref table for gages that do not occur in NWIS. Locations may NOT be located exactly on the NHM segment. The POIs' assigned segment is displayed in the popup window when the gage icon is clicked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be13534-cd82-437d-88e5-815b33d0ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hru_gdf(GIS_format, \n",
    "                   model_dir,\n",
    "                   nhru_params,\n",
    "                   nhru_nmonths_params,\n",
    "                   ):\n",
    "    \"\"\"\n",
    "    Creates hru gdf for selected hru parameters from the parameter file.\n",
    "    Selected in notebook 0a.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of bynhru parameters to retrieve for the Notebook interactive maps.\n",
    "    hru_params = [\n",
    "        \"hru_lat\",  # the latitude if the hru centroid\n",
    "        \"hru_lon\",  # the longitude if the hru centroid\n",
    "        \"hru_area\",\n",
    "        \"hru_segment_nhm\",  # The nhm_id of the segment recieving flow from the HRU\n",
    "    ]\n",
    "    cal_hru_params = nhru_params + nhru_nmonths_params\n",
    "    gdb_hru_params = hru_params + nhru_params + nhru_nmonths_params\n",
    "    \n",
    "    \"\"\"\n",
    "    Projections are ascribed geometry from the HRUs geodatabase (GIS). \n",
    "    The NHM uses the NAD 1983 USGS Contiguous USA Albers projection EPSG# 102039. \n",
    "    The geometry units of this projection are not useful for many notebook packages. \n",
    "    The geodatabases are reprojected to World Geodetic System 1984.\n",
    "\n",
    "    Options:\n",
    "        crs = 3857, WGS 84 / Pseudo-Mercator - Spherical Mercator, Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI.\n",
    "        *crs = 4326, WGS 84 - WGS84 - World Geodetic System 1984, used in GPS\n",
    "    \"\"\"\n",
    "    crs = 4326\n",
    "    \n",
    "    if GIS_format == \".gpkg\":\n",
    "        hru_gdb = gpd.read_file(\n",
    "            f\"{model_dir}/GIS/model_layers.gpkg\", layer=\"nhru\"\n",
    "        )  # Reads HRU file to Geopandas.\n",
    "    \n",
    "    if GIS_format == \".shp\":\n",
    "        hru_gdb = gpd.read_file(\n",
    "            f\"{model_dir}/GIS/model_nhru.shp\"\n",
    "        )  # Reads HRU file to Geopandas.\n",
    "        hru_gdb = hru_gdb.set_index(\"nhm_id\", drop=False).fillna(\n",
    "            0\n",
    "        )  # Set an index for HRU geodatabase.\n",
    "        hru_gdb.index.name = \"index\"  # Index column must be renamed of the hru\n",
    "    \n",
    "    hru_gdb = hru_gdb.to_crs(crs)  # reprojects to the defined crs projection\n",
    "    \n",
    "    # Create a dataframe for parameter values\n",
    "    first = True\n",
    "    for vv in gdb_hru_params:\n",
    "        if (\n",
    "            first\n",
    "        ):  # this creates the first iteration for the following iterations to concantonate to\n",
    "            df = pdb.get_dataframe(vv)\n",
    "            first = False\n",
    "        else:\n",
    "            df = pd.concat([df, pdb.get_dataframe(vv)], axis=1)  # , ignore_index=True)\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df[\"model_idx\"] = (\n",
    "        df.index + 1\n",
    "    )  #'model_idx' created here is the order of the parameters in the parameter file.\n",
    "    # df\n",
    "    \n",
    "    # Join the HRU params values to the HRU geodatabase using Merge\n",
    "    hru_gdb = pd.merge(df, hru_gdb, on=\"nhm_id\")\n",
    "    \n",
    "    # Create a Goepandas GeoDataFrame for the HRU geodatabase\n",
    "    hru_gdf = gpd.GeoDataFrame(\n",
    "        hru_gdb, geometry=\"geometry\"\n",
    "    )\n",
    "    \n",
    "    return hru_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca7e861-1ca3-4d23-acc5-c0a879ac9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segment_gdf(GIS_format, \n",
    "                       model_dir,\n",
    "                      ):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates segment gdf for selected segment parameters from the parameter file.\n",
    "    Selected in notebook 0a.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List of parameters values to retrieve for the segments.\n",
    "    seg_params = [\"tosegment_nhm\", \"tosegment\", \"seg_length\", \"obsin_segment\"]\n",
    "    \n",
    "    \"\"\"\n",
    "    Projections are ascribed geometry from the HRUs geodatabase (GIS). \n",
    "    The NHM uses the NAD 1983 USGS Contiguous USA Albers projection EPSG# 102039. \n",
    "    The geometry units of this projection are not useful for many notebook packages. \n",
    "    The geodatabases are reprojected to World Geodetic System 1984.\n",
    "\n",
    "    Options:\n",
    "        crs = 3857, WGS 84 / Pseudo-Mercator - Spherical Mercator, Google Maps, OpenStreetMap, Bing, ArcGIS, ESRI.\n",
    "        *crs = 4326, WGS 84 - WGS84 - World Geodetic System 1984, used in GPS\n",
    "    \"\"\"\n",
    "    crs = 4326\n",
    "\n",
    "    if GIS_format == \".gpkg\":\n",
    "        seg_gdb = gpd.read_file(\n",
    "            f\"{model_dir}/GIS/model_layers.gpkg\", layer=\"nsegment\"\n",
    "        ).fillna(\n",
    "            0\n",
    "        )  # Reads segemnt file to Geopandas.\n",
    "    \n",
    "    if GIS_format == \".shp\":\n",
    "        seg_gdb = gpd.read_file(f\"{model_dir}/GIS/model_nsegment.shp\").fillna(0)\n",
    "        seg_gdb = seg_gdb.set_index(\n",
    "            \"nhm_seg\", drop=False\n",
    "        )  # Set an index for segment geodatabase(GIS)\n",
    "        seg_gdb.index.name = \"index\"  # Index column must be renamed of the hru\n",
    "    \n",
    "    seg_gdb = seg_gdb.to_crs(crs)  # reprojects to the defined crs projection\n",
    "    \n",
    "    # Create a dataframe for parameter values\n",
    "    first = True\n",
    "    for vv in seg_params:\n",
    "        if first:\n",
    "            df = pdb.get_dataframe(vv)\n",
    "            first = False\n",
    "        else:\n",
    "            df = pd.concat([df, pdb.get_dataframe(vv)], axis=1)  # , ignore_index=True)\n",
    "    \n",
    "    df.reset_index(inplace=True)\n",
    "    df[\"model_idx\"] = df.index + 1\n",
    "    df.index.name = \"index\"  # Index column must be renamed\n",
    "    \n",
    "    # Join the HRU params values to the HRU geodatabase using Merge\n",
    "    seg_gdb = pd.merge(df, seg_gdb, on=\"nhm_seg\")\n",
    "    \n",
    "    # Create a Goepandas GeoDataFrame for the HRU geodatabase\n",
    "    seg_gdf = gpd.GeoDataFrame(seg_gdb, geometry=\"geometry\")\n",
    "    \n",
    "    return seg_gdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f31fa1-ca9d-48ee-b1dc-2d05639edf08",
   "metadata": {},
   "source": [
    "## 4. Create POI DataFrame (poi_df) for gages (poi_gages) included in the parameter file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430bb9c-c255-44f2-baf6-c98aa539a1e1",
   "metadata": {},
   "source": [
    "#### Create a dataframe of all POI-related parameters from the parameter file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2363dccf-f9f0-4625-b5cb-5403deb9bb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NHM_helpers.NHM_Assist_utilities import fetch_nwis_gage_info\n",
    "\n",
    "def create_poi_gdf(pdb,\n",
    "                   control,\n",
    "                   \n",
    "    \n",
    "):\n",
    "    \"\"\"\n",
    "    Create a dataframe of all POI-related parameters from the parameter file.\n",
    "    \"\"\"\n",
    "    \n",
    "    poi = pdb[\"poi_gage_id\"].as_dataframe\n",
    "    poi = poi.merge(pdb[\"poi_gage_segment\"].as_dataframe, left_index=True, right_index=True)\n",
    "    poi = poi.merge(pdb[\"poi_type\"].as_dataframe, left_index=True, right_index=True)\n",
    "    poi = poi.merge(\n",
    "        pdb[\"nhm_seg\"].as_dataframe, left_on=\"poi_gage_segment\", right_index=True\n",
    "    )\n",
    "    \n",
    "    st_date = control.start_time\n",
    "    en_date = control.end_time\n",
    "    \n",
    "    # Make a list if the HUC2 region(s) the subbasin intersects for NWIS queries\n",
    "    huc2_gdf = gpd.read_file(\"./data_dependencies/HUC2/HUC2.shp\").to_crs(crs)\n",
    "    model_domain_regions = list((huc2_gdf.clip(hru_gdf).loc[:][\"huc2\"]).values)\n",
    "    # print(model_domain_regions)\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a dataframe of all avialable NWIS gages and thier metadata in the model domain (spatial and temporal) \n",
    "    that have at least 90 days of streamflow obervations.\n",
    "    \"\"\"\n",
    "    \n",
    "    nwis_gages_aoi = fetch_nwis_gage_info(\n",
    "        nwis_gage_nobs_min,\n",
    "        model_domain_regions,\n",
    "        st_date,\n",
    "        en_date,\n",
    "        hru_gdf,\n",
    "        nwis_gages_file,\n",
    "        crs,\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a dataframe for poi_gages from the parameter file with NWIS gage information data.\n",
    "    \"\"\"\n",
    "    poi = poi.merge(nwis_gages_aoi, left_on=\"poi_gage_id\", right_on=\"poi_id\", how=\"left\")\n",
    "    poi_df = pd.DataFrame(poi)  # Creates a Pandas DataFrame\n",
    "    \n",
    "    return poi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ae690-f22d-46c9-af17-237604436615",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create default_gages.csv for your model extraction.\n",
    "### NHM-Assist notebooks will display gages using the default gages file (default_gages.csv), if a modified gages file (gages.csv) is lacking.\n",
    "##### By default, this file will be composed of \n",
    "1) the gages listed in the parameter file (poi_gages), and\n",
    "2) all streamflow gages from NWIS in the model domain that have at least 90 days of streamflow obervations.\n",
    "\n",
    "###### Note: Time-series data for streamflow observations will be collected using this gage list and the time range in the control file.\n",
    "###### Note: Initially, all gages listed in the parameter file exist in NWIS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd47350-20a8-4b5b-87b8-99456be9dfae",
   "metadata": {},
   "source": [
    "### Make a dataframe of the non-NWIS gages (if present) in the parameter file (poi_gages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93512624-515f-4976-8bb7-05bdc68194d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe of the gages in the parameter file that are not USGS gages in NWIS\n",
    "if pd.isnull(poi_df[\"poi_agency\"]).values.any():\n",
    "    non_NWIS_gages_from_poi_df = poi_df.loc[pd.isnull(poi_df[\"poi_agency\"])]\n",
    "    non_NWIS_gages_from_poi_df.drop(\n",
    "        columns=[\"poi_id\", \"nhm_seg\", \"poi_gage_segment\", \"poi_type\"], inplace=True\n",
    "    )\n",
    "    non_NWIS_gages_from_poi_df.rename(columns={\"poi_gage_id\": \"poi_id\"}, inplace=True)\n",
    "    # non_NWIS_gages_from_poi_df\n",
    "\n",
    "    non_NWIS_gages_from_poi_list = non_NWIS_gages_from_poi_df.poi_id.to_list()\n",
    "    non_NWIS_gages_from_poi_df_txt = f\"[bold red]WARNING:[/bold red] {len(non_NWIS_gages_from_poi_list)} of the {len(poi_df)} gages in your parameter file were not found in NWIS. Please check for missing gage information in the default_gages file.\"\n",
    "else:\n",
    "    non_NWIS_gages_from_poi_df_txt = (\n",
    "        f\"All {len(poi_df)} gages in the parameter file were found in NWIS.\"\n",
    "    )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c4eba-8bd3-40f6-8b3a-2043354fef4f",
   "metadata": {},
   "source": [
    "### Make a dataframe for all gages in the parameter file and additional NWIS gages found in the model domain\n",
    "### Note: the NWIS gages in the poi_df (gages in the parameter file) should be in NWIS_sites_aoi df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa730ed-b61f-4352-aac0-a6ddaaefad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sta_file_col_order = [\n",
    "    \"poi_id\",\n",
    "    \"poi_agency\",\n",
    "    \"poi_name\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"drainage_area\",\n",
    "    \"drainage_area_contrib\",\n",
    "    #'nhm_seg', 'poi_gage_segment', 'poi_type'\n",
    "]\n",
    "if pd.isnull(poi_df[\"poi_agency\"]).values.any():\n",
    "    temp = pd.concat([nwis_gages_aoi, non_NWIS_gages_from_poi_df], ignore_index=True)\n",
    "    temp2 = temp[sta_file_col_order]\n",
    "\n",
    "else:\n",
    "    temp = nwis_gages_aoi.copy()\n",
    "    temp2 = temp[sta_file_col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d484280-803f-46fb-b4b1-680c320a0313",
   "metadata": {},
   "source": [
    "### Save the default station file (gage file) as a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690621fc-467f-4efb-8bb6-d87ab88d7ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2.to_csv(default_gages_file, index=False)\n",
    "# temp2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d330c350-62f0-43f5-8cd9-5695f07fa69f",
   "metadata": {},
   "source": [
    "## Read modified gages file (gages.csv).\n",
    "If there are gages in the parameter file that are not in NWIS (USGS gages), then latitude, longitude, and poi_name must be provided from another source, and appended to the \"default_gages.csv\" file. Once editing is complete, that file can be renamed \"gages.csv\"and will be used as the gages file. If NO gages.csv is made, the default_gages.csv will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796099f9-97c2-4aae-b03a-8c8b4e8d658b",
   "metadata": {},
   "source": [
    "#### If an improved/edited gages.csv file exists, then the poi_df metadata will be updated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952662d6-201f-405e-8e0c-cc9e7d2c9e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updates the non-usgs gages in the poi dataframe with metadata from the stations file (that was added or edited)\n",
    "if gages_file.exists():\n",
    "    for idx, row in poi_df.iterrows():\n",
    "        if pd.isnull(row[\"poi_id\"]):\n",
    "            new_poi_id = row[\"poi_gage_id\"]\n",
    "            new_lat = gages_df.loc[\n",
    "                gages_df.index == row[\"poi_gage_id\"], \"latitude\"\n",
    "            ].values[0]\n",
    "            new_lon = gages_df.loc[\n",
    "                gages_df.index == row[\"poi_gage_id\"], \"longitude\"\n",
    "            ].values[0]\n",
    "            new_poi_agency = gages_df.loc[\n",
    "                gages_df.index == row[\"poi_gage_id\"], \"poi_agency\"\n",
    "            ].values[0]\n",
    "            new_poi_name = gages_df.loc[\n",
    "                gages_df.index == row[\"poi_gage_id\"], \"poi_name\"\n",
    "            ].values[0]\n",
    "\n",
    "            poi_df.loc[idx, \"latitude\"] = new_lat\n",
    "            poi_df.loc[idx, \"longitude\"] = new_lon\n",
    "            poi_df.loc[idx, \"poi_id\"] = new_poi_id\n",
    "            poi_df.loc[idx, \"poi_agency\"] = new_poi_agency\n",
    "            poi_df.loc[idx, \"poi_name\"] = new_poi_name\n",
    "\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24ccd9c-9f59-4ceb-a789-b68f782bc59a",
   "metadata": {},
   "source": [
    "#### If an improved/edited gages.csv file exists, NHM-Assist notebooks will display gages using the modified gage file (gages.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9854913-2a83-42d4-a3f6-9065773a25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in station file columns needed (You may need to tailor this to the particular file.\n",
    "col_names = [\n",
    "    \"poi_id\",\n",
    "    \"poi_agency\",\n",
    "    \"poi_name\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"drainage_area\",\n",
    "    \"drainage_area_contrib\",\n",
    "]\n",
    "col_types = [np.str_, np.str_, np.str_, float, float, float, float]\n",
    "cols = dict(\n",
    "    zip(col_names, col_types)\n",
    ")  # Creates a dictionary of column header and datatype called below.\n",
    "\n",
    "\n",
    "if gages_file.exists():\n",
    "\n",
    "    nwis_gages_aoi = pd.read_csv(nwis_gages_file, dtype=cols)\n",
    "    gages_df = pd.read_csv(gages_file)\n",
    "\n",
    "    # Make poi_id the index\n",
    "    gages_df.set_index(\"poi_id\", inplace=True)\n",
    "    agencies_txt = \", \".join(f\"{item}\" for item in list(set(poi_df.poi_agency)))\n",
    "\n",
    "    con.print(\n",
    "        f\"[bold]Create hydrofabric files:\\n\",\n",
    "        f\"\\n{non_NWIS_gages_from_poi_df_txt}\",\n",
    "        f\"\\nNHM-Assist notebooks will display gages using the modified gage file (gages.csv).\",\n",
    "        f\"\\nThe default gage file includes {len(gages_df)} gages from {agencies_txt} listed in the parameter file, and found in NWIS for the model domain.\",\n",
    "    )\n",
    "else:\n",
    "    gages_df = pd.read_csv(default_gages_file, dtype=cols)\n",
    "\n",
    "    # Make poi_id the index\n",
    "    gages_df.set_index(\"poi_id\", inplace=True)\n",
    "    agencies_txt = \", \".join(f\"{item}\" for item in list(set(poi_df.poi_agency)))\n",
    "\n",
    "    con.print(\n",
    "        f\"[bold]Create hydrofabric files:\\n\",\n",
    "        f\"\\n{non_NWIS_gages_from_poi_df_txt}\",\n",
    "        f\"\\nNHM-Assist notebooks will display gages using the default gages file (default_gages.csv).\",\n",
    "        f\"\\nThe default gage file includes {len(gages_df)} gages from {agencies_txt} listed in the parameter file, and found in NWIS for the model domain.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d5208f-1414-48f2-b0cd-70fe23976c44",
   "metadata": {},
   "source": [
    "#### CHECK: All pois in the poi_df with missing metadata for lat, lon, and poi_name will be dropped from the poi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75d273d-d738-4d26-b708-f145172d6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print warning and drop poi's with missing data for lat, lon, and poi_name\n",
    "missing_meta_df = poi_df[\n",
    "    poi_df[[\"latitude\", \"longitude\", \"poi_name\"]].isna().any(axis=1)\n",
    "]  # poi_df[poi_df.isna().any(axis=1)]\n",
    "missing_meta_list = list(missing_meta_df.poi_gage_id.values)\n",
    "\n",
    "if missing_meta_list:\n",
    "    missing_meta_txt = f\"[bold red]WARNING:[\\bold red] Gage {missing_meta_list} missing metadata and will be dropped from the poi_gdf and will not be plotted on maps in the notebook.\\\n",
    "    To avoid this, open up the default_gages file and add the missing metadata: latitude, longitude, and gage name.\"\n",
    "else:\n",
    "    missing_meta_txt = f\"[bold green]All gages in the default gage file have metadata and will be plotted on maps in the notebook.\"\n",
    "\n",
    "\n",
    "# poi_df.notna(inplace=True, ignore_index=False)\n",
    "# poi_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "poi_gdf = gpd.GeoDataFrame(\n",
    "    poi_df,\n",
    "    geometry=gpd.points_from_xy(poi_df.longitude, poi_df.latitude),\n",
    "    crs=crs,\n",
    ").dropna(subset=[\"latitude\", \"longitude\", \"poi_name\"])\n",
    "\n",
    "con.print(missing_meta_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46945a80-d0fb-4827-a7d0-124f8bcfaecc",
   "metadata": {},
   "source": [
    "### Useful variables from poi_df, gages_df, and nwis_gages_aoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1f076-d1e6-435d-8c4f-fc245aec0f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gages_list = gages_df.index.to_list()\n",
    "nwis_gages_aoi_list = nwis_gages_aoi.poi_id.to_list()\n",
    "nwis_gages_in_gages_list = list(set(nwis_gages_aoi_list) & set(gages_list))\n",
    "additional_gages = list(set(gages_list) - set(poi_df.poi_id))\n",
    "nwis_gages_in_additional_gages_list = list(\n",
    "    set(nwis_gages_aoi_list) & set(additional_gages)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d06f8b-a41f-4e3f-bfa1-dd29310d140d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
