{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa55d1-68ef-4373-b5ea-bdce0726271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run \"0a_Workspace_setup.ipynb\"\n",
    "%run \"0b_Create_poi_files.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f71b5e-9f32-4fe9-b58d-b3b05df41a1d",
   "metadata": {},
   "source": [
    "## Scraper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe4b03-ac14-42a4-99f4-ad22f13e5256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def owrd_scraper(station_nbr, start_date, end_date):\n",
    "    # f string the args into the urldf\n",
    "    url = f\"https://apps.wrd.state.or.us/apps/sw/hydro_near_real_time/hydro_download.aspx?station_nbr={station_nbr}&start_date={start_date}&end_date={end_date}&dataset=MDF&format=html\"\n",
    "\n",
    "    # open and decode the url\n",
    "    resource = request.urlopen(url)\n",
    "    content = resource.read().decode(resource.headers.get_content_charset())\n",
    "\n",
    "    # Ugly parsing between pre tags\n",
    "    # initializing substrings\n",
    "    sub1 = \"<pre>\"\n",
    "    sub2 = \"</pre>\"\n",
    "\n",
    "    # getting index of substrings\n",
    "    idx1 = content.index(sub1)\n",
    "    idx2 = content.index(sub2)\n",
    "\n",
    "    res = \"\"\n",
    "    # getting elements in between\n",
    "    for idx in range(idx1 + len(sub1), idx2):\n",
    "        res = res + content[idx]\n",
    "\n",
    "    # make and return the pandas df\n",
    "\n",
    "    # NOTE:\n",
    "    # Read in the csv file taking care to set the data types exactly. This is important for stability and functionality.\n",
    "    # This should be done everytime the databases are read into this and future notebooks!\n",
    "\n",
    "    col_names = [\n",
    "        \"station_nbr\",\n",
    "        \"record_date\",\n",
    "        \"mean_daily_flow_cfs\",\n",
    "        #'published_status',\n",
    "        #'estimated',\n",
    "        #'revised',\n",
    "        #'download_date',\n",
    "    ]\n",
    "    col_types = [\n",
    "        np.str_,\n",
    "        np.str_,\n",
    "        float,\n",
    "        # np.str_,\n",
    "        # np.str_,\n",
    "        # float,\n",
    "        # np.str_,\n",
    "    ]\n",
    "    cols = dict(\n",
    "        zip(col_names, col_types)\n",
    "    )  # Creates a dictionary of column header and datatype called below.\n",
    "\n",
    "    owrd_df = pd.read_csv(StringIO(res), sep=\"\\t\", header=0, dtype=cols)\n",
    "    owrd_df\n",
    "    return owrd_df\n",
    "\n",
    "\n",
    "def ecy_scrape(station, ecy_years):\n",
    "    ecy_df_list = []\n",
    "    for ecy_year in ecy_years:\n",
    "        url = f\"https://apps.ecology.wa.gov/ContinuousFlowAndWQ/StationData/Prod/{station}/{station}_{ecy_year}_DSG_DV.txt\"\n",
    "        try:\n",
    "            # The string that is to be searched\n",
    "            key = \"DATE\"\n",
    "\n",
    "            # Opening the file and storing its data into the variable lines\n",
    "            with urlopen(url) as file:\n",
    "                lines = file.readlines()\n",
    "            \n",
    "\n",
    "            # Going over each line of the file\n",
    "            dateline = []\n",
    "            for number, line in enumerate(lines, 1):\n",
    "\n",
    "                # Condition true if the key exists in the line\n",
    "                # If true then display the line number\n",
    "                if key in str(line):\n",
    "                    dateline.append(number)\n",
    "                    # print(f'{key} is at line {number}')\n",
    "            # df = pd.read_csv(url, skiprows=11, sep = '\\s{3,}', on_bad_lines='skip', engine = 'python')  # looks for at least three spaces as separator\n",
    "            df = pd.read_fwf(\n",
    "                url, skiprows=dateline[0]\n",
    "            )  # seems to handle formatting for No Data and blanks together, above option is thrown off by blanks\n",
    "            # df['Day'] = pd.to_numeric(df['Day'], errors='coerce') # day col to numeric\n",
    "            # df = df[df['Day'].notna()].astype({'Day': int}) #\n",
    "            # df = df.drop('Day.1', axis=1)\n",
    "            if len(df.columns) == 3:\n",
    "                df.columns = [\"time\", \"discharge\", \"Quality\"]\n",
    "            elif len(df.columns) == 4:\n",
    "                df.columns = [\"time\", \"utc\", \"discharge\", \"Quality\"]\n",
    "                df.drop(\"utc\", axis=1, inplace=True)\n",
    "            try:\n",
    "                df.drop(\n",
    "                    \"Quality\", axis=1, inplace=True\n",
    "                )  # drop quality for now, might use to filter later\n",
    "            except KeyError:\n",
    "                print(f\"no Quality for {station} {ecy_year}\")\n",
    "            df[\"time\"] = pd.to_datetime(df[\"time\"], errors=\"coerce\")\n",
    "            df = df.dropna(subset=[\"time\"])\n",
    "            df[\"poi_id\"] = station\n",
    "            df[\"discharge\"] = pd.to_numeric(df[\"discharge\"], errors=\"coerce\")\n",
    "            # specify data types\n",
    "            dtype_map = {\"poi_id\": str, \"time\": \"datetime64[ns]\"}\n",
    "            df = df.astype(dtype_map)\n",
    "\n",
    "            df.set_index([\"poi_id\", \"time\"], inplace=True)\n",
    "            # next two lines are new if this breaks...\n",
    "            idx = pd.IndexSlice\n",
    "            df = df.loc[\n",
    "                idx[:, ecy_start_date:ecy_end_date], :\n",
    "            ]  # filters to the date range\n",
    "            df[\"agency_id\"] = \"ECY\"\n",
    "\n",
    "            ecy_df_list.append(df)\n",
    "            print(f\"good year {ecy_year}\")\n",
    "            print(url)\n",
    "        except HTTPError:\n",
    "            pass\n",
    "        except ValueError as ex:\n",
    "            print(ex)\n",
    "            print(ecy_year)\n",
    "    if len(df) != 0:\n",
    "        ecy_df = pd.concat(ecy_df_list)\n",
    "        # ecy_df[\"discharge_cfs\"] = pd.to_numeric(ecy_df[\"discharge_cfs\"], errors = 'coerce')\n",
    "        return ecy_df\n",
    "    else:\n",
    "        print(f\"No data for station {station} for data range {ecy_years}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8143b765-accd-437c-9fd3-d07d8d1edef8",
   "metadata": {},
   "source": [
    "#### Set start and end dates for streamflow daily time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b9c1d-5251-4201-ae7d-56cebbaa1da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start and end times in format 'mm/dd/YYYY' from pws control process reading control file\n",
    "\n",
    "start_date = pd.to_datetime(str(control.start_time)).strftime(\"%m/%d/%Y\")\n",
    "end_date = pd.to_datetime(str(control.end_time)).strftime(\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666f939a-733f-4828-8e0b-46a10755114b",
   "metadata": {},
   "source": [
    "# Get daily streamflow data for gages\n",
    "##### We will not use the list of gages in the model paramter file, but will use the gages lsited in the gages_file.csv. This The reasoning: there may be multiple observation datasets that are associated with a single segment outflow (gage_poi), and, in the parameter file, only one poi_gage can be associated with a segment. We want a streamflow data set that is more inclusive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbf08fb-beba-42de-b54b-a9c8f4d72a0e",
   "metadata": {},
   "source": [
    "## Create Xarray dataset with station information (indexed by poi_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd84225-36c9-4260-9d04-2c6215b7431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Stations file now becomes the source of the gage metadata\n",
    "xr_station_info = xr.Dataset.from_dataframe(gages_df)\n",
    "# xr_station_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05df29-3f38-4714-a427-41a9cb4c56e8",
   "metadata": {},
   "source": [
    "## Download OWRD streamflow data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea717dd-42ce-4fe5-a6f0-43933c30d251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(owrd_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb713d95-683a-46c5-a026-468504c41d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# CPU times: user 3min 49s, sys: 51.3 s, total: 4min 40s\n",
    "# Wall time: 35min 18s\n",
    "if owrd_domain == \"no\":\n",
    "    con.print(\"There are no OWRD gages in the model domain.\")\n",
    "\n",
    "if owrd_domain == \"yes\":\n",
    "    owrd_cache_file = model_dir / \"notebook_output_files\" / \"nc_files\" / \"owrd_cache.nc\"\n",
    "    lst = []\n",
    "\n",
    "    if owrd_cache_file.exists():\n",
    "        with xr.open_dataset(owrd_cache_file) as owrd_ds:\n",
    "            # ds = xr.open_dataset(owrd_cache_file)\n",
    "            owrd_df = owrd_ds.to_dataframe()\n",
    "        print(\n",
    "            \"Cached copy of OWRD data exists. To re-download the data remove the cache file.\"\n",
    "        )\n",
    "        print(owrd_cache_file)\n",
    "        del owrd_ds\n",
    "    else:\n",
    "        with Progress() as progress:\n",
    "            task = progress.add_task(\"[red]Downloading...\", total=len(poi_df))\n",
    "\n",
    "            # Get the streamflow data. This will return a list of lists\n",
    "            #         a time series for each gage..[[gage0], [gage1], [gage2]...[gagen]]\n",
    "            for ii in gages_df.index:\n",
    "                lst.append(owrd_scraper(ii, start_date, end_date))\n",
    "                progress.update(task, advance=1)\n",
    "\n",
    "        owrd_df = pd.concat(lst)  # Converts the list of df's to a single df\n",
    "\n",
    "        # Rename field names\n",
    "        field_map = {\n",
    "            \"station_nbr\": \"poi_id\",\n",
    "            \"record_date\": \"time\",\n",
    "            \"mean_daily_flow_cfs\": \"discharge\",\n",
    "            \"station_name\": \"poi_name\",\n",
    "        }\n",
    "        owrd_df.rename(columns=field_map, inplace=True)\n",
    "\n",
    "        # Change the datatype for poi_id\n",
    "        dtype_map = {\"poi_id\": str, \"time\": \"datetime64[ns]\"}\n",
    "        owrd_df = owrd_df.astype(dtype_map)\n",
    "\n",
    "        # Drop the columns we don't need\n",
    "        drop_cols = [\"download_date\", \"estimated\", \"revised\", \"published_status\"]\n",
    "        owrd_df.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "        # Add new fields\n",
    "        owrd_df[\"agency_id\"] = \"OWRD\"  # Creates tags for all OWRD daily streamflow data\n",
    "\n",
    "        # See a multi-index\n",
    "        owrd_df.set_index([\"poi_id\", \"time\"], inplace=True)\n",
    "\n",
    "        owrd_ds = xr.Dataset.from_dataframe(owrd_df)\n",
    "\n",
    "        # Set attributes for the variables\n",
    "        owrd_ds[\"discharge\"].attrs = {\"units\": \"ft3 s-1\", \"long_name\": \"discharge\"}\n",
    "        owrd_ds[\"poi_id\"].attrs = {\n",
    "            \"role\": \"timeseries_id\",\n",
    "            \"long_name\": \"Point-of-Interest ID\",\n",
    "            \"_Encoding\": \"ascii\",\n",
    "        }\n",
    "        owrd_ds[\"agency_id\"].attrs = {\"_Encoding\": \"ascii\"}\n",
    "\n",
    "        # Set encoding\n",
    "        # See 'String Encoding' section at https://crusaderky-xarray.readthedocs.io/en/latest/io.html\n",
    "        owrd_ds[\"poi_id\"].encoding.update(\n",
    "            {\"dtype\": \"S15\", \"char_dim_name\": \"poiid_nchars\"}\n",
    "        )\n",
    "\n",
    "        owrd_ds[\"time\"].encoding.update(\n",
    "            {\n",
    "                \"_FillValue\": None,\n",
    "                \"standard_name\": \"time\",\n",
    "                \"calendar\": \"standard\",\n",
    "                \"units\": \"days since 1940-01-01 00:00:00\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        owrd_ds[\"agency_id\"].encoding.update(\n",
    "            {\"dtype\": \"S5\", \"char_dim_name\": \"agency_nchars\"}\n",
    "        )\n",
    "\n",
    "        # Add fill values to the data variables\n",
    "        var_encoding = dict(_FillValue=netCDF4.default_fillvals.get(\"f4\"))\n",
    "\n",
    "        for cvar in owrd_ds.data_vars:\n",
    "            if cvar not in [\"agency_id\"]:\n",
    "                owrd_ds[cvar].encoding.update(var_encoding)\n",
    "\n",
    "        # add global attribute metadata\n",
    "        owrd_ds.attrs = {\n",
    "            \"Description\": \"Streamflow data for PRMS\",\n",
    "            \"FeatureType\": \"timeSeries\",\n",
    "        }\n",
    "\n",
    "        # Write the dataset to a netcdf file\n",
    "        owrd_ds.to_netcdf(owrd_cache_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0511ef3-c8f2-48b0-b23f-1a7e38e7761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# owrd_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec6b617-8bad-4e54-a751-4dee1a6957b5",
   "metadata": {},
   "source": [
    "## Download ECY streamflow data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9434224-4ba9-43e0-ab33-8fd7a4bd4964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dates for ecy_scrape\n",
    "if owrd_domain == \"no\":\n",
    "    con.print(\"There are no ECY gages in the model domain.\")\n",
    "\n",
    "if owrd_domain == \"yes\":\n",
    "    ecy_cache_file = model_dir / \"notebook_output_files\" / \"nc_files\" / \"ecy_cache.nc\"\n",
    "\n",
    "    ecy_start_date = dt.datetime.strptime(start_date, \"%m/%d/%Y\").strftime(\"%Y-%m-%d\")\n",
    "    ecy_end_date = dt.datetime.strptime(end_date, \"%m/%d/%Y\").strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # create a year range for ecy data\n",
    "    ecy_start_year = dt.datetime.strptime(start_date, \"%m/%d/%Y\")\n",
    "    ecy_end_year = dt.datetime.strptime(end_date, \"%m/%d/%Y\")\n",
    "\n",
    "    # add 1 year to date range because ecy is water year, add another because for range is not inclusive\n",
    "    ecy_years = range(ecy_start_year.year, ecy_end_year.year + 2)\n",
    "    # ecy_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae20a4c-5403-4240-833b-5d62b9f3f1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find ecology gages in the gage list, they are 6 alpha numeric, third is char\n",
    "if owrd_domain == \"no\":\n",
    "    pass\n",
    "\n",
    "if owrd_domain == \"yes\":\n",
    "    ecy_gages = []\n",
    "    # gage_list = ['01C070', '03D050', '32B075', '32B090', '32B100', '32A100', '32A105', '32A120'] # change this list to that from the stations list\n",
    "    gage_list = gages_df.index.to_list()\n",
    "    for i in gage_list:\n",
    "        # if len(i) == 6 and i.matches(\"^[A-Z]{1}\\\\d{3}\")\n",
    "        if len(i) == 6 and i[0:2].isdigit() and i[2].isalpha() and i[4:6].isdigit():\n",
    "            ecy_gages.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    # ecy_gages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88222235-20e3-444c-b748-10efc85ce31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if ecy_gages:\n",
    "    ecy_df_list = []\n",
    "\n",
    "    if ecy_cache_file.exists():\n",
    "        with xr.open_dataset(ecy_cache_file) as ecy_ds:\n",
    "            ecy_df = ecy_ds.to_dataframe()\n",
    "        print(\n",
    "            \"Cached copy of ECY data exists. To re-download the data remove the cache file.\"\n",
    "        )\n",
    "        print(ecy_cache_file)\n",
    "        del ecy_ds\n",
    "    else:\n",
    "        with Progress() as progress:\n",
    "            task = progress.add_task(\"[red]Downloading...\", total=len(ecy_gages))\n",
    "\n",
    "            # Get the streamflow data. This will return a list of lists\n",
    "            #         a time series for each gage..[[gage0], [gage1], [gage2]...[gagen]]\n",
    "            for ii in ecy_gages:\n",
    "                try:\n",
    "                    ecy_df_list.append(ecy_scrape(ii, ecy_years))\n",
    "                    progress.update(task, advance=1)\n",
    "                except UnboundLocalError:\n",
    "                    print(f\"No data for {ii}\")\n",
    "                    progress.update(task, advance=1)\n",
    "                    pass\n",
    "\n",
    "        ecy_df = pd.concat(ecy_df_list)  # Converts the list of df's to a single df\n",
    "\n",
    "        # set the multiIndex\n",
    "        # ecy_df.set_index(['poi_id', 'time'], inplace=True)\n",
    "        # # Change the datatype for poi_id\n",
    "\n",
    "        ecy_df = ecy_df[\n",
    "            ~ecy_df.index.duplicated(keep=\"first\")\n",
    "        ]  # overlap in ecy records for 10-1, drop duplicates for xarray\n",
    "        # ecy_df = ecy_df.join(ecy_station_info_df, how='inner') # bring in station info\n",
    "\n",
    "        # Add new fields\n",
    "        ecy_df[\"agency_id\"] = \"ECY\"  # Creates tags for all ECY daily streamflow data\n",
    "\n",
    "        ecy_ds = xr.Dataset.from_dataframe(ecy_df)\n",
    "\n",
    "        # Set attributes for the variables\n",
    "        ecy_ds[\"discharge\"].attrs = {\"units\": \"ft3 s-1\", \"long_name\": \"discharge\"}\n",
    "        ecy_ds[\"poi_id\"].attrs = {\n",
    "            \"role\": \"timeseries_id\",\n",
    "            \"long_name\": \"Point-of-Interest ID\",\n",
    "            \"_Encoding\": \"ascii\",\n",
    "        }\n",
    "        ecy_ds[\"agency_id\"].attrs = {\"_Encoding\": \"ascii\"}\n",
    "\n",
    "        # Set encoding\n",
    "        # See 'String Encoding' section at https://crusaderky-xarray.readthedocs.io/en/latest/io.html\n",
    "        ecy_ds[\"poi_id\"].encoding.update(\n",
    "            {\"dtype\": \"S15\", \"char_dim_name\": \"poiid_nchars\"}\n",
    "        )\n",
    "\n",
    "        ecy_ds[\"time\"].encoding.update(\n",
    "            {\n",
    "                \"_FillValue\": None,\n",
    "                \"standard_name\": \"time\",\n",
    "                \"calendar\": \"standard\",\n",
    "                \"units\": \"days since 1940-01-01 00:00:00\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        ecy_ds[\"agency_id\"].encoding.update(\n",
    "            {\"dtype\": \"S5\", \"char_dim_name\": \"agency_nchars\"}\n",
    "        )\n",
    "\n",
    "        # Add fill values to the data variables\n",
    "        var_encoding = dict(_FillValue=netCDF4.default_fillvals.get(\"f4\"))\n",
    "\n",
    "        for cvar in ecy_ds.data_vars:\n",
    "            if cvar not in [\"agency_id\"]:\n",
    "                ecy_ds[cvar].encoding.update(var_encoding)\n",
    "\n",
    "        # add global attribute metadata\n",
    "        ecy_ds.attrs = {\n",
    "            \"Description\": \"Streamflow data for PRMS\",\n",
    "            \"FeatureType\": \"timeSeries\",\n",
    "        }\n",
    "\n",
    "        # ecy_ds = (ecy_ds.sel(time=slice(ecy_start_date, ecy_end_date))) # get the values only between the start and end date\n",
    "\n",
    "        # Write the dataset to a netcdf file\n",
    "        ecy_ds.to_netcdf(ecy_cache_file)\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f5f52-4717-469a-af0e-efe6a7eb5b57",
   "metadata": {},
   "source": [
    "Note: ecy_ds is going outside of the date range. use the time slice using ecy_start_date and ecy_end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a1213-b3e3-4c3a-8f1d-9516990dfffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ecy_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1ebd2-1623-4350-993c-cb0ee666a377",
   "metadata": {},
   "source": [
    "## Download NWIS streamflow data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28263251-bfde-458a-80bd-ad7d4beff553",
   "metadata": {},
   "source": [
    "### Make a list of NWIS gages in the model domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b1e48-8dae-451d-90de-b01278ac9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_gages_aoi.set_index(\"poi_id\", inplace=True)\n",
    "nwis_gages_list = nwis_gages_aoi.index.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0aed15-bb4e-4af3-9781-1ac6ce5c3614",
   "metadata": {},
   "source": [
    "### Change start/end date format to match nwis retrieval tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bb4430-c2a9-4b6e-b4c2-81e367019689",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwis_start = dt.datetime.strptime(start_date, \"%m/%d/%Y\").strftime(\"%Y-%m-%d\")\n",
    "nwis_end = dt.datetime.strptime(end_date, \"%m/%d/%Y\").strftime(\"%Y-%m-%d\")\n",
    "con.print(\n",
    "    f\"We will retrieve NWIS records beginning on {nwis_start} and ending on {nwis_end}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb329531-7a09-4a46-9abb-c53088a2b2d0",
   "metadata": {},
   "source": [
    "### Retrieve daily streamflow data from NWIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a7d7d-97a9-4877-a7a2-69bb4d7629b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427842dd-9938-4d19-a324-ecbe0a2fd492",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    NWIS_tmp = []\n",
    "\n",
    "    with Progress() as progress:\n",
    "        task = progress.add_task(\"[red]Downloading...\", total=len(gages_df))\n",
    "\n",
    "        for ii in gages_df.index:\n",
    "            if str(ii) in nwis_gages_list:\n",
    "                NWISgage_data = nwis.get_record(\n",
    "                    sites=(str(ii)), service=\"dv\", start=nwis_start, end=nwis_end\n",
    "                )\n",
    "                NWIS_tmp.append(NWISgage_data)\n",
    "            else:\n",
    "                pass\n",
    "            progress.update(task, advance=1)\n",
    "\n",
    "    NWIS_ds = pd.concat(NWIS_tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5181f8d-8eb3-4fd1-b7eb-5e5cb0f18b21",
   "metadata": {},
   "source": [
    "### Clean up the NWIS timeseries and add 'agency_id' variable (value= 'USGS') for each daily obs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5816c02-0d10-4e7e-8ab4-ce46f108769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    # we only need site_no and discharge (00060_Mean)\n",
    "    NWIS_ds = NWIS_ds[[\"site_no\", \"00060_Mean\"]].copy()\n",
    "    NWIS_ds[\"agency_id\"] = \"USGS\"\n",
    "\n",
    "    NWIS_ds = NWIS_ds.tz_localize(None)\n",
    "    NWIS_ds.reset_index(inplace=True)\n",
    "\n",
    "    # rename cols to match other df\n",
    "    NWIS_ds.rename(\n",
    "        columns={\"datetime\": \"time\", \"00060_Mean\": \"discharge\", \"site_no\": \"poi_id\"},\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    NWIS_ds.set_index([\"poi_id\", \"time\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2eb9d-ab8a-44d0-8875-3468e2ab50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NWIS_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d83b65-47c8-4fcb-99a3-e2382932329f",
   "metadata": {},
   "source": [
    "## Combine NWIS with OWRD and ECY daily streamflow (if nedded)\n",
    "note: all NWIS data is mirrored the OWRD database without any primary source tags. This section will also etermine the original source or each daily observation and create a tag for each daily record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8c0c8-4a5f-46b7-8131-18c175e17dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# owrd_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83541a18-42bc-4391-b93f-3f0f2640ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    if owrd_domain == \"no\":\n",
    "        streamflow_df = NWIS_ds.copy()\n",
    "\n",
    "    if owrd_domain == \"yes\":\n",
    "        # Merge NWIS and OWRD\n",
    "        NWIS_OWRD_joined = pd.concat([NWIS_ds, owrd_df])  # Join the two datasets\n",
    "        # Drop duplicated indexes, keeping the first occurence (USGS occurs first)\n",
    "        # try following this thing: https://saturncloud.io/blog/how-to-drop-duplicated-index-in-a-pandas-dataframe-a-complete-guide/#:~:text=Pandas%20provides%20the%20drop_duplicates(),names%20to%20the%20subset%20parameter.\n",
    "        NWIS_OWRD_clean = NWIS_OWRD_joined[\n",
    "            ~NWIS_OWRD_joined.index.duplicated(keep=\"first\")\n",
    "        ]\n",
    "\n",
    "        # Merge with ECY\n",
    "        if ecy_gages:\n",
    "            NWIS_OWRD_ECY_joined = pd.concat([NWIS_OWRD_clean, ecy_df])\n",
    "        else:\n",
    "            NWIS_OWRD_ECY_joined = NWIS_OWRD_clean\n",
    "        # drop duplicated indexes, keeping the last occurence (ECY occurs last). Only dups now should be ecy\n",
    "        # try following this thing: https://saturncloud.io/blog/how-to-drop-duplicated-index-in-a-pandas-dataframe-a-complete-guide/#:~:text=Pandas%20provides%20the%20drop_duplicates(),names%20to%20the%20subset%20parameter.\n",
    "        NWIS_OWRD_ECY_clean = NWIS_OWRD_ECY_joined[\n",
    "            ~NWIS_OWRD_ECY_joined.index.duplicated(keep=\"last\")\n",
    "        ]\n",
    "\n",
    "        streamflow_df = NWIS_OWRD_ECY_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b096ec79-14e1-4c20-8a5d-73e216aa86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamflow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5177c-8ae3-4750-a464-7f026dbb2ff9",
   "metadata": {},
   "source": [
    "#### Poteniatlly important code later for checking stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40222976-eb9a-4b41-a8b9-a9ef0e708c95",
   "metadata": {},
   "source": [
    "## Create Xarray dataset with streamflow date set (indexed by poi_id and time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb5c736-9f60-4789-aa96-71edabef1bd7",
   "metadata": {},
   "source": [
    "## Make and Xarray data set for NWIS, OWRD, and WA Ecology data and encode with station information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5cf100-3148-4700-8451-55145bad13cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    xr_streamflow_only = xr.Dataset.from_dataframe(streamflow_df)\n",
    "    xr_streamflow = xr.merge(\n",
    "        [xr_streamflow_only, xr_station_info], combine_attrs=\"drop_conflicts\"\n",
    "    )\n",
    "    test_poi = xr_streamflow.poi_id.values[2]\n",
    "    # xr_streamflow.discharge.sel[poi_id = '32N070'].to_dataframe()\n",
    "    xr_streamflow.agency_id.sel(poi_id=test_poi).to_dataframe().agency_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e54c1-02c1-4bb1-abae-3f8502a102cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd09d459-57ad-4f0a-b14c-1deb0e74f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    # Set attributes for the variables\n",
    "    xr_streamflow[\"discharge\"].attrs = {\"units\": \"ft3 s-1\", \"long_name\": \"discharge\"}\n",
    "    xr_streamflow[\"drainage_area\"].attrs = {\n",
    "        \"units\": \"mi2\",\n",
    "        \"long_name\": \"Drainage Area\",\n",
    "    }\n",
    "    xr_streamflow[\"drainage_area_contrib\"].attrs = {\n",
    "        \"units\": \"mi2\",\n",
    "        \"long_name\": \"Effective drainage area\",\n",
    "    }\n",
    "    xr_streamflow[\"latitude\"].attrs = {\n",
    "        \"units\": \"degrees_north\",\n",
    "        \"long_name\": \"Latitude\",\n",
    "    }\n",
    "    xr_streamflow[\"longitude\"].attrs = {\n",
    "        \"units\": \"degrees_east\",\n",
    "        \"long_name\": \"Longitude\",\n",
    "    }\n",
    "    xr_streamflow[\"poi_id\"].attrs = {\n",
    "        \"role\": \"timeseries_id\",\n",
    "        \"long_name\": \"Point-of-Interest ID\",\n",
    "        \"_Encoding\": \"ascii\",\n",
    "    }\n",
    "    xr_streamflow[\"poi_name\"].attrs = {\n",
    "        \"long_name\": \"Name of POI station\",\n",
    "        \"_Encoding\": \"ascii\",\n",
    "    }\n",
    "    xr_streamflow[\"time\"].attrs = {\"standard_name\": \"time\"}\n",
    "    xr_streamflow[\"poi_agency\"].attrs = {\"_Encoding\": \"ascii\"}\n",
    "    xr_streamflow[\"agency_id\"].attrs = {\"_Encoding\": \"ascii\"}\n",
    "\n",
    "    # Set encoding\n",
    "    # See 'String Encoding' section at https://crusaderky-xarray.readthedocs.io/en/latest/io.html\n",
    "    xr_streamflow[\"poi_id\"].encoding.update(\n",
    "        {\"dtype\": \"S15\", \"char_dim_name\": \"poiid_nchars\"}\n",
    "    )\n",
    "\n",
    "    xr_streamflow[\"time\"].encoding.update(\n",
    "        {\n",
    "            \"_FillValue\": None,\n",
    "            \"calendar\": \"standard\",\n",
    "            \"units\": \"days since 1940-01-01 00:00:00\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    xr_streamflow[\"latitude\"].encoding.update({\"_FillValue\": None})\n",
    "    xr_streamflow[\"longitude\"].encoding.update({\"_FillValue\": None})\n",
    "\n",
    "    xr_streamflow[\"agency_id\"].encoding.update(\n",
    "        {\"dtype\": \"S5\", \"char_dim_name\": \"agency_nchars\"}\n",
    "    )\n",
    "\n",
    "    xr_streamflow[\"poi_name\"].encoding.update(\n",
    "        {\"dtype\": \"S50\", \"char_dim_name\": \"poiname_nchars\"}\n",
    "    )\n",
    "\n",
    "    xr_streamflow[\"poi_agency\"].encoding.update(\n",
    "        {\"dtype\": \"S5\", \"char_dim_name\": \"mro_nchars\", \"_FillValue\": \"\"}\n",
    "    )\n",
    "    # Add fill values to the data variables\n",
    "    var_encoding = dict(_FillValue=netCDF4.default_fillvals.get(\"f4\"))\n",
    "\n",
    "    for cvar in xr_streamflow.data_vars:\n",
    "        if xr_streamflow[cvar].dtype != object and cvar not in [\n",
    "            \"latitude\",\n",
    "            \"longitude\",\n",
    "        ]:\n",
    "            xr_streamflow[cvar].encoding.update(var_encoding)\n",
    "\n",
    "    # add global attribute metadata\n",
    "    xr_streamflow.attrs = {\n",
    "        \"Description\": \"Streamflow data for PRMS\",\n",
    "        \"FeatureType\": \"timeSeries\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0704a-9d3c-43a0-afd6-39648728430a",
   "metadata": {},
   "source": [
    "## Assign EFC values to the Xarray dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e655ba-b41c-4329-8880-e1d36f9bbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attributes for the EFC-related variables\n",
    "attributes = {\n",
    "    \"efc\": {\n",
    "        \"dtype\": np.int32,\n",
    "        \"attrs\": {\n",
    "            \"long_name\": \"Extreme flood classification\",\n",
    "            \"_FillValue\": -1,\n",
    "            \"valid_range\": [1, 5],\n",
    "            \"flag_values\": [1, 2, 3, 4, 5],\n",
    "            \"flag_meanings\": \"large_flood small_flood high_flow_pulse low_flow extreme_low_flow\",\n",
    "        },\n",
    "    },\n",
    "    \"ri\": {\n",
    "        \"dtype\": np.float32,\n",
    "        \"attrs\": {\n",
    "            \"long_name\": \"Recurrence interval\",\n",
    "            \"_FillValue\": 9.96921e36,\n",
    "            \"units\": \"year\",\n",
    "        },\n",
    "    },\n",
    "    \"high_low\": {\n",
    "        \"dtype\": np.int32,\n",
    "        \"attrs\": {\n",
    "            \"long_name\": \"Discharge classification\",\n",
    "            \"_FillValue\": -1,\n",
    "            \"valid_range\": [1, 3],\n",
    "            \"flag_values\": [1, 2, 3],\n",
    "            \"flag_meanings\": \"low_flow ascending_limb descending_limb\",\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee7e6ac-233e-48dd-8770-d510f086de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_enc = {}\n",
    "# for var, info in attributes.items():\n",
    "#     # Add the variable\n",
    "#     xr_merged[var] = xr.zeros_like(xr_merged['discharge'], dtype=info['dtype'])\n",
    "\n",
    "#     var_enc[var] = {'zlib': True, 'complevel': 2}\n",
    "\n",
    "#     # Take care of the attributes\n",
    "#     del xr_merged[var].attrs['units']\n",
    "\n",
    "#     for kk, vv in info['attrs'].items():\n",
    "#         if kk == '_FillValue':\n",
    "#             var_enc[var][kk] = vv\n",
    "#         else:\n",
    "#             xr_merged[var].attrs[kk] = vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd91918-d903-41c3-aba9-bd88ecb1eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "    var_enc = {}\n",
    "    for var, info in attributes.items():\n",
    "        # Add the variable\n",
    "        xr_streamflow[var] = xr.zeros_like(\n",
    "            xr_streamflow[\"discharge\"], dtype=info[\"dtype\"]\n",
    "        )\n",
    "\n",
    "        var_enc[var] = {\"zlib\": True, \"complevel\": 2}\n",
    "\n",
    "        # Take care of the attributes\n",
    "        del xr_streamflow[var].attrs[\"units\"]\n",
    "\n",
    "        for kk, vv in info[\"attrs\"].items():\n",
    "            if kk == \"_FillValue\":\n",
    "                var_enc[var][kk] = vv\n",
    "            else:\n",
    "                xr_streamflow[var].attrs[kk] = vv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a2efd-9a3f-487e-9c1d-e00a4475377c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if output_netcdf_filename.exists():\n",
    "    pass\n",
    "else:\n",
    "\n",
    "    flow_col = \"discharge\"\n",
    "\n",
    "    for pp in xr_streamflow.poi_id.data:\n",
    "        try:\n",
    "            df = efc(\n",
    "                xr_streamflow.discharge.sel(poi_id=pp).to_dataframe(), flow_col=flow_col\n",
    "            )\n",
    "\n",
    "            # Add EFC values to the xarray dataset for the poi\n",
    "            xr_streamflow[\"efc\"].sel(poi_id=pp).data[:] = df.efc.values\n",
    "            xr_streamflow[\"high_low\"].sel(poi_id=pp).data[:] = df.high_low.values\n",
    "            xr_streamflow[\"ri\"].sel(poi_id=pp).data[:] = df.ri.values\n",
    "        except TypeError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a9c0d-8972-4143-a2d5-40a77fe295e5",
   "metadata": {},
   "source": [
    "### Plot individual POI with EFC highlighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815b37b4-0382-4417-82bf-3c01c5535882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Helper functions for plotting, developed by P.A. Norton, USGS Dakota Water Science Center\n",
    "# def plot_efc(df, flow_col):\n",
    "#     fig, ax = plt.subplots(nrows=1, figsize=(15, 5), layout='tight')\n",
    "\n",
    "#     mkrsize = 9.1\n",
    "\n",
    "#     # cmap = ListedColormap(['#000000', '#cb4335', '#f8c471', '#95a5a6', '#76d7c4', '#154360'])\n",
    "#     cmap = ['#000000', '#cb4335', '#f8c471', '#95a5a6', '#76d7c4', '#154360']\n",
    "#     labels = ['', 'Large flood', 'Small flood', 'High flow pulse', 'Low flow', 'Extreme low flow']\n",
    "\n",
    "#     ax.plot(df.index, df[flow_col], c='grey', lw=.5, alpha=0.5)\n",
    "\n",
    "#     for xx in range(1,6):\n",
    "#         sdf = df[df['efc'] == xx]\n",
    "\n",
    "#         if sdf.shape[0] != 0:\n",
    "#             ax.scatter(sdf.index, sdf[flow_col], c=cmap[xx], s=mkrsize, lw=0, alpha=0.7, label=labels[xx])\n",
    "\n",
    "#     ax.set_title('Extreme flood classifications', fontsize=10)\n",
    "#     ax.legend(loc='upper left', framealpha=0.5)\n",
    "\n",
    "\n",
    "# def plot_high_low(df, flow_col):\n",
    "#     fig, ax = plt.subplots(nrows=1, figsize=(15, 5), layout='tight')\n",
    "\n",
    "#     mkrsize = 9.1\n",
    "\n",
    "#     cmap = ['', '#00cc66','#ff9933','#9933ff']\n",
    "#     labels = ['', 'Low flow', 'Ascending limb', 'Descending limb']\n",
    "\n",
    "#     ax.plot(df.index, df[flow_col], c='grey', lw=.5, alpha=0.5)\n",
    "\n",
    "#     for xx in range(1,4):\n",
    "#         sdf = df[df['high_low'] == xx]\n",
    "\n",
    "#         if sdf.shape[0] != 0:\n",
    "#             ax.scatter(sdf.index, sdf[flow_col], c=cmap[xx], s=mkrsize, lw=0, alpha=0.7, label=labels[xx])\n",
    "\n",
    "#     ax.set_title('High/Low classifications', fontsize=10)\n",
    "#     ax.legend(loc='upper left', framealpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f079c-b639-4e90-9f19-096decad6375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xr_streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b73ff-8486-42d5-a2d6-abc680aadd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cpoi_id =  xr_streamflow.poi_id.values[12]\n",
    "\n",
    "# # Get subset of data to plot\n",
    "# #df_motion = df_motion.sort_index().loc['2020-07-10 13:55' : '2020-07-10 14:25.5', :]\n",
    "\n",
    "# xr_streamflow = xr_streamflow.sortby('time', ascending=True)# bug fix for xarray\n",
    "# ds_sub = xr_streamflow.sel(poi_id=cpoi_id, time=slice('1980-10-01', '2022-12-31'))\n",
    "# ds_sub = ds_sub.to_dataframe()\n",
    "\n",
    "# # Or use entire dataset\n",
    "# #ds_sub = xr_merged.sel(poi_id=cpoi_id).to_dataframe()\n",
    "\n",
    "# # Plot EFC\n",
    "# plot_efc(ds_sub, flow_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0634e13-d764-4b53-8c08-7899b8e01d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_high_low(ds_sub, flow_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656dfc8-a21c-47c8-9d3c-b393248d66f8",
   "metadata": {},
   "source": [
    "## Write the Xarray data set to a netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc6695-fb2f-440d-b45e-a265a423abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if output_netcdf_filename.exists():\n",
    "    con.print(\n",
    "        f\"The output file already exists. [bold]To re-write[/] the file, please [bold]remove[/] the existing file:\"\n",
    "    )\n",
    "    con.print(f\"[bright_magenta]{output_netcdf_filename}[/]\")\n",
    "else:\n",
    "    xr_streamflow.to_netcdf(output_netcdf_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa92ef3-aa63-471c-99e5-b16d744a7c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xr_streamflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958e0252-f936-469a-af65-8e81c547ac1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
