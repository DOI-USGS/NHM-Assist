{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5fa2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./0_workspace_setup.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f8a63e",
   "metadata": {},
   "source": [
    "# Updating Climate Drivers with gdptools\n",
    "\n",
    "The Willamette River modeling domain currently uses climate data from 1979â€“2022. The gridMET dataset, which is updated daily with a one-day lag, allows us to regularly update our climate drivers with the latest available data.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook introduces tools and workflows for updating climate drivers in the current modeling domain. It covers:\n",
    "\n",
    "1. An overview of the [`gdptools`](https://gdptools.readthedocs.io/en/develop/) package, which spatially interpolates gridded climate data to the polygonal modeling domain (HRUs) using areal-intersection weights.\n",
    "2. A workflow for updating climate drivers using:\n",
    "   - **`gdptools`** ([repo](https://code.usgs.gov/wma/nhgf/toolsteam/gdptools))\n",
    "   - **`pyPRMS`** ([repo](https://github.com/DOI-USGS/pyPRMS/tree/development/docs))\n",
    "\n",
    "## `gdptools` Package\n",
    "\n",
    "`gdptools` is a Python package for spatially interpolating gridded data to a polygonal fabric using areal weighting. It is used here to interpolate [gridMET climate data](https://www.climatologylab.org/gridmet.html) to the Willamette River modeling domain. `gdptools` was also used to create the original climate drivers for this domain.\n",
    "\n",
    "While `gdptools` provides the initial spatial interpolation, further post-processing is sometimes required:\n",
    "1. Renaming variables and dimensions for compatibility with PRMS, pyPRMS, and pyWatershed.\n",
    "2. Filling missing data, if the gridded dataset does not fully overlap the modeling domain. For the Willamette River domain, gridMET coverage is complete, but we include the filling step for completeness.\n",
    "\n",
    "### Source and Target Data\n",
    "\n",
    "- **Source data:** gridMET gridded climate data.\n",
    "- **Target data:** The modeling domain, defined in `0_workspace_setup.ipynb` as `./domain_data/willamette_river/model_layers.gpkg`.\n",
    "\n",
    "`gdptools` provides an interface to the ClimateR-Catalog, a collection of gridded climate datasets and metadata. gridMET is included in this catalog.\n",
    "\n",
    "### Working with the ClimateR-Catalog\n",
    "\n",
    "The [ClimateR-Catalog](https://github.com/mikejohnson51/climateR-catalogs) contains metadata for gridded climate datasets, including URLs, variable names, and coordinate information. We use the latest Parquet catalog file from [this release](https://github.com/mikejohnson51/climateR-catalogs/releases/download/June-2024/catalog.parquet), read it into a pandas DataFrame, and filter for the gridMET dataset. We then create a dictionary mapping variables of interest to their catalog entries.\n",
    "\n",
    "`gdptools` provides the `ClimRCatData` data class, which we use in the workflow below. The steps are:\n",
    "1. Read the catalog into a pandas DataFrame.\n",
    "2. Search for the relevant gridMET data.\n",
    "3. Use the `ClimRCatData` class to access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1ec532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import hvplot.xarray\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "\n",
    "from gdptools import WeightGen\n",
    "from gdptools import AggGen\n",
    "from gdptools import ClimRCatData\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb8545",
   "metadata": {},
   "source": [
    "## Inspect the existing climate driver file\n",
    "In this section, we will read the existing climate driver file and inspect its contents. The file is located at `./domain_data/willamette_river/cbh.nc`. We will use the `xarray` library to read the NetCDF file and display its structure.\n",
    "\n",
    "Note the existing time bounds used in the Willamette River modeling domain are from 1979-2022. We will update these bounds to include the latest available data from gridMET. Also note the data variable names, `tmax`, `tmin`, `prcp`, their units and dimension names, particularly `nhru`, as we will have to rename and convert our initial data processed by `gdptools` in our post-processing steps below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10426531",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_ds = xr.open_dataset(\"domain_data/willamette_river/cbh.nc\")\n",
    "existing_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7547cc",
   "metadata": {},
   "source": [
    "## 1. Read the ClimageR-Catalog into a pandas DataFrame and parameterize the `ClimRCatData` class, which we use to represent our source data.\n",
    "\n",
    "The ClimateR-Catalog is a collection of gridded climate datasets and associated metadata. We will read the latest catalog file into a pandas DataFrame for further processing. In addition to the parquet file, there is also a JSON file available and for first time users it can be useful to open the file in a text editor that supports [JSON](https://github.com/mikejohnson51/climateR-catalogs/releases/download/June-2024/catalog.json) formatting to see the structure of the catalog.  Also the gdptools documentation has a [table](https://gdptools.readthedocs.io/en/develop/#example-catalog-datasets) of common datasets available in the catalog.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73acd31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "climrcat_url = \"https://github.com/mikejohnson51/climateR-catalogs/releases/download/June-2024/catalog.parquet\"\n",
    "climrcat_df = pd.read_parquet(climrcat_url)\n",
    "climrcat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc27a11",
   "metadata": {},
   "source": [
    "### Pandas DataFrame Query Language\n",
    "\n",
    "Pandas provides a powerful query language that allows us to filter and manipulate data in a DataFrame. We can use the `query()` method to filter rows based on specific conditions. For example, if we want to filter the DataFrame for rows where the `id` column is equal to `gridmet`, we can use the following syntax where `@` is used to reference variables in the query string:\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849238b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_id = \"gridmet\"\n",
    "gridmet_df = climrcat_df.query(\"id == @ _id\")\n",
    "gridmet_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e351a280",
   "metadata": {},
   "source": [
    "### Further Processing of the ClimateR-Catalog Data for use with `gdptools` `ClimRCatData`\n",
    "\n",
    "Once we have filtered the DataFrame for the gridMET dataset, we can create a dictionary that maps variable names to their corresponding catalog entries. This will allow us to easily access the data for each variable when using `gdptools`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411a84ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of climateR-catalog values for each variable\n",
    "tvars = [\"tmmn\", \"tmmx\", \"pr\"]\n",
    "cat_params = [\n",
    "    gridmet_df.query(\"id == @ _id & variable == @ _var\").to_dict(orient=\"records\")[0]for _var in tvars\n",
    "]\n",
    "\n",
    "cat_dict = dict(zip(tvars, cat_params))\n",
    "\n",
    "# Output an example of the cat_param.json entry for \"aet\".\n",
    "cat_dict.get(\"tmmn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce7e79",
   "metadata": {},
   "source": [
    "### Read in the target data file and inspect its contents\n",
    "\n",
    "The `ClimRCatData` class requires a target data file that defines the polygonal modeling domain (HRUs). We will read the target data file located at `./domain_data/willamette_river/model_layers.gpkg` and inspect its contents. This file contains the geometry of the HRUs, which will be used for spatial interpolation of the climate data.  In addition, we need the column header used to identify the HRU geometry, in this case `model_hru_idx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_gdf = gpd.read_file(\n",
    "    \"./domain_data/willamette_river/GIS/model_layers.gpkg\", layer=\"nhru\"\n",
    ")\n",
    "target_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fba892",
   "metadata": {},
   "source": [
    "### Parameterize the `ClimRCatData` class\n",
    "\n",
    "We use this data class to further parameterize WeightGen and AggGen classes in `gdptools` for generating areal weights and aggregating data, respectively. Set the period the time bounds for the data we want to process, in this case we will we will update the existing data through then of 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20080a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_data = ClimRCatData(\n",
    "    cat_dict=cat_dict,\n",
    "    f_feature=target_gdf,\n",
    "    id_feature=\"model_hru_idx\",\n",
    "    period=[\"2023-01-01\", \"2024-12-31\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad7cf9",
   "metadata": {},
   "source": [
    "## 2. Generate Areal Weights with `gdptools`\n",
    "\n",
    "The Areal Weights Generator (`WeightGen`) in `gdptools` is used to create areal weights for spatial interpolation of gridded climate data to the polygonal modeling domain (HRUs). The weights are calculated based on the intersection of the gridded data with the HRU geometries. The weights are a table representing the target column header id, the gridded data cell ids (i and j indexes), and the normalized areal weights for each cell within each HRU.  As long as the source and target data are the same used in generating the weights, the weights can be reused for subsequent updates to the climate drivers.\n",
    "\n",
    "> Note: The `calculate_weights` method returns the weights as a pandas DataFrame, and also saves the weights to a CSV file for later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7efee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdptools_path = Path(\"./domain_data/willamette_river/gdptools\")\n",
    "if not gdptools_path.exists():\n",
    "    gdptools_path.mkdir(parents=True)\n",
    "weights_file = gdptools_path / \"gridmet_Wn_wghts.csv\"\n",
    "if not weights_file.exists():\n",
    "    wght_gen = WeightGen(\n",
    "        user_data=user_data,\n",
    "        method=\"serial\",\n",
    "        output_file=weights_file,\n",
    "        weight_gen_crs=5070,\n",
    "    )\n",
    "\n",
    "wghts = wght_gen.calculate_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c50e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wghts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26369e2c",
   "metadata": {},
   "source": [
    "A simple check on the generated weights is to group the weights by the target column header id and sum the weights. The sum should equal 1 for each target id, indicating that the weights are normalized.  Those target geometries with weights that sum to less than 1 indicate that the gridded data does not fully cover the HRU geometry, and we will need to fill those gaps in the post-processing step.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9da285",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_wghts = wghts.groupby(\"model_hru_idx\").sum().reset_index()\n",
    "sum_wghts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cd243",
   "metadata": {},
   "source": [
    "### Inspect the generated weights\n",
    "\n",
    "Here we provide a quick inspection of the generated weights to ensure there are no HRUs with weights that sum to less than 1. If there are, we will need to fill those gaps in the post-processing step.\n",
    "\n",
    "> Note: In this case there are no HRUs with weights that sum to less than 1, indicating that the gridded data fully covers the HRU geometries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1627142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your tolerance (atol=absolute, rtol=relative)\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Boolean mask: which values are NOT close to 1 (outside tolerance)\n",
    "not_close_to_1 = ~np.isclose(sum_wghts['wght'], 1.0, atol=tolerance)\n",
    "\n",
    "print(sum_wghts[not_close_to_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57afef4",
   "metadata": {},
   "source": [
    "## 3. Aggregate the Climate Data with `gdptools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3378d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_out_path = Path(\"./domain_data/willamette_river/gdptools\")\n",
    "agg_gen = AggGen(\n",
    "    user_data=user_data,\n",
    "    stat_method=\"masked_mean\",\n",
    "    agg_engine=\"serial\",\n",
    "    agg_writer=\"netcdf\",\n",
    "    weights= \"./domain_data/willamette_river/gdptools/gridmet_Wn_wghts.csv\",\n",
    "    out_path=agg_out_path,\n",
    "    file_prefix=\"cbh_2024_temp\",\n",
    ")\n",
    "ngdf, ds_out = agg_gen.calculate_agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dd6a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_climate_ds = xr.open_dataset(\"domain_data/willamette_river/gdptools/cbh_2024_temp.nc\")\n",
    "new_climate_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa55e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_new = new_climate_ds.rename(\n",
    "    {\n",
    "        \"daily_minimum_temperature\": \"tmin\",\n",
    "        \"daily_maximum_temperature\": \"tmax\",\n",
    "        \"precipitation_amount\": \"precip\",\n",
    "        \"model_hru_idx\": \"hruid\",\n",
    "    }\n",
    ")\n",
    "ds_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pint_xarray\n",
    "\n",
    "# 1. Quantify the dataset: attaches Pint units to each variable\n",
    "quantified = ds_new.pint.quantify()\n",
    "\n",
    "# 2. Perform unit conversions in-place on a copy, maintaining Dataset structure\n",
    "#    Use assign() so you do not have to break out variables\n",
    "quantified_converted = quantified.assign(\n",
    "    tmin=quantified[\"tmin\"].pint.to(\"degF\"),\n",
    "    tmax=quantified[\"tmax\"].pint.to(\"degF\"),\n",
    "    precip=quantified[\"precip\"].pint.to(\"inch\")  # \"inch\" or \"inches\" depending on your registry\n",
    ")\n",
    "\n",
    "# 3. Dequantify: converts Pint Quantities back to vanilla xarray, puts units in .attrs\n",
    "dequantified = quantified_converted.pint.dequantify()\n",
    "\n",
    "# 4. Save to NetCDF\n",
    "dequantified.to_netcdf(\"./domain_data/willamette_river/gdptools/cbh_2024.nc\")\n",
    "\n",
    "# 5. (Optional) Inspect units to verify\n",
    "for v in dequantified.data_vars:\n",
    "    print(f\"{v}: {dequantified[v].attrs.get('units', 'No units attribute')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc262feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dequantified"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
